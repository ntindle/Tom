{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoGPT Documentation","text":"<p>Welcome to the AutoGPT Documentation.</p> <p>The AutoGPT project consists of four main components:</p> <ul> <li>The Agent \u2013 also known as just \"AutoGPT\"</li> <li>The Benchmark \u2013 AKA <code>agbenchmark</code></li> <li>The Forge</li> <li>The Frontend</li> </ul> <p>To tie these together, we also have a CLI at the root of the project.</p>"},{"location":"#agent","title":"\ud83e\udd16 Agent","text":"<p>\ud83d\udcd6 About AutoGPT \u2002|\u2002 \ud83d\udd27 Setup \u2002|\u2002 \ud83d\udcbb Usage</p> <p>The heart of AutoGPT, and the project that kicked it all off: a semi-autonomous agent powered by LLMs to execute any task for you*.</p> <p>We continue to develop this project with the goal of providing access to AI assistance to the masses, and building the future transparently and together.</p> <ul> <li> <p>\ud83d\udca1 Explore - See what AI can do and be inspired by a glimpse of the future.</p> </li> <li> <p>\ud83d\ude80 Build with us - We welcome any input, whether it's code or ideas for new features or improvements! Join us on Discord and find out how you can join in on the action.</p> </li> </ul> <p>* it isn't quite there yet, but that is the ultimate goal that we are still pursuing</p>"},{"location":"#benchmark","title":"\ud83c\udfaf Benchmark","text":"<p>\ud83d\uddd2\ufe0f Readme</p> <p>Measure your agent's performance! The <code>agbenchmark</code> can be used with any agent that supports the agent protocol, and the integration with the project's CLI makes it even easier to use with AutoGPT and forge-based agents. The benchmark offers a stringent testing environment. Our framework allows for autonomous, objective performance evaluations, ensuring your agents are primed for real-world action.</p> <ul> <li> <p>\ud83d\udce6 <code>agbenchmark</code> on Pypi</p> </li> <li> <p>\ud83d\udd0c Agent Protocol Standardization - AutoGPT uses the agent protocol from the AI Engineer Foundation to ensure compatibility with many agents, both from within and outside the project.</p> </li> </ul>"},{"location":"#forge","title":"\ud83c\udfd7\ufe0f Forge","text":"<p>\ud83d\udcd6 Introduction \u2002|\u2002 \ud83d\ude80 Quickstart</p> <p>Forge your own agent! The Forge is a ready-to-go template for your agent application. All the boilerplate code is already handled, letting you channel all your creativity into the things that set your agent apart.</p> <ul> <li>\ud83d\udee0\ufe0f Building with Ease - We've set the groundwork so you can focus on your agent's personality and capabilities. Comprehensive tutorials are available here.</li> </ul>"},{"location":"#frontend","title":"\ud83d\udcbb Frontend","text":"<p>\ud83d\uddd2\ufe0f Readme</p> <p>An easy-to-use and open source frontend for any Agent Protocol-compliant agent.</p> <ul> <li> <p>\ud83c\udfae User-Friendly Interface - Manage your agents effortlessly.</p> </li> <li> <p>\ud83d\udd04 Seamless Integration - Smooth connectivity between your agent and our benchmarking system.</p> </li> </ul>"},{"location":"#cli","title":"\ud83d\udd27 CLI","text":"<p>The project CLI makes it easy to use all of the components in the repo, separately or together. To install its dependencies, simply run <code>./run setup</code>, and you're ready to go!</p> <pre><code>$ ./run\nUsage: cli.py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent      Commands to create, start and stop agents\n  benchmark  Commands to start the benchmark and list tests and categories\n  setup      Installs dependencies needed for your system.\n</code></pre> <p>Common commands:</p> <ul> <li><code>./run agent start autogpt</code> \u2013 runs the AutoGPT agent</li> <li><code>./run agent create &lt;name&gt;</code> \u2013 creates a new Forge-based agent project at <code>agents/&lt;name&gt;</code></li> <li><code>./run benchmark start &lt;agent&gt;</code> \u2013 benchmarks the specified agent</li> </ul> <p>\ud83e\udd14 Join the AutoGPT Discord server for any queries: discord.gg/autogpt</p>"},{"location":"#glossary-of-terms","title":"Glossary of Terms","text":"<ul> <li>Repository: Space where your project resides.</li> <li>Forking: Copying a repository under your account.</li> <li>Cloning: Making a local copy of a repository.</li> <li>Agent: The AutoGPT you'll create and develop.</li> <li>Benchmarking: Testing your agent's skills in the Forge.</li> <li>Forge: The template for building your AutoGPT agent.</li> <li>Frontend: The UI for tasks, logs, and task history.</li> </ul>"},{"location":"code-of-conduct/","title":"Code of Conduct","text":"<p>../../CODE_OF_CONDUCT.md</p>"},{"location":"contributing/","title":"Contribution guide","text":"<p>../../CONTRIBUTING.md</p>"},{"location":"AutoGPT/","title":"AutoGPT Agent","text":"<p>\ud83d\udd27 Setup \u2002|\u2002 \ud83d\udcbb User guide \u2002|\u2002 \ud83d\udc19 GitHub</p> <p>Location: <code>autogpt/</code> in the GitHub repo</p> <p>AutoGPT was conceived when OpenAI published their GPT-4 model accompanied by a paper outlining the advanced reasoning and task-solving abilities of the model. The concept was (and still is) fairly simple: let an LLM decide what to do over and over, while feeding the results of its actions back into the prompt. This allows the program to iteratively and incrementally work towards its objective.</p> <p>The fact that this program is able to execute actions on behalf of its user makes it an agent. In the case of AutoGPT, the user still has to authorize every action, but as the project progresses we'll be able to give the agent more autonomy and only require consent for select actions.</p> <p>AutoGPT is a generalist agent, meaning it is not designed with a specific task in mind. Instead, it is designed to be able to execute a wide range of tasks across many disciplines, as long as it can be done on a computer.</p>"},{"location":"AutoGPT/#coming-soon","title":"Coming soon","text":"<ul> <li>How does AutoGPT work?</li> <li>What can I use AutoGPT for?</li> <li>What does the future of AutoGPT look like?</li> </ul>"},{"location":"AutoGPT/share-your-logs/","title":"Share your logs with us to help improve AutoGPT","text":"<p>Do you notice weird behavior with your agent? Do you have an interesting use case? Do you have a bug you want to report? Follow the steps below to enable your logs and upload them. You can include these logs when making an issue report or discussing an issue with us.</p>"},{"location":"AutoGPT/share-your-logs/#enable-debug-logs","title":"Enable Debug Logs","text":"<p>Activity, Error, and Debug logs are located in <code>./logs</code></p> <p>To print out debug logs:</p> <pre><code>./autogpt.sh --debug     # on Linux / macOS\n\n.\\autogpt.bat --debug    # on Windows\n\ndocker-compose run --rm auto-gpt --debug    # in Docker\n</code></pre>"},{"location":"AutoGPT/share-your-logs/#inspect-and-share-logs","title":"Inspect and share logs","text":"<p>You can inspect and share logs via e2b. </p> <ol> <li>Go to autogpt.e2b.dev and sign in.</li> <li>You'll see logs from other members of the AutoGPT team that you can inspect.</li> <li>Or you upload your own logs. Click on the \"Upload log folder\" button and select the debug logs dir that you generated. Wait a 1-2 seconds and the page reloads.</li> <li>You can share logs via sharing the URL in your browser. </li> </ol>"},{"location":"AutoGPT/share-your-logs/#add-tags-to-logs","title":"Add tags to logs","text":"<p>You can add custom tags to logs for other members of your team. This is useful if you want to indicate that the agent is for example having issues with challenges.</p> <p>E2b offers 3 types of severity:</p> <ul> <li>Success</li> <li>Warning</li> <li>Error</li> </ul> <p>You can name your tag any way you want.</p>"},{"location":"AutoGPT/share-your-logs/#how-to-add-a-tag","title":"How to add a tag","text":"<ol> <li> <p>Click on the \"plus\" button on the left from the logs folder name.</p> <p></p> </li> <li> <p>Type the name of a new tag.</p> </li> <li> <p>Select the severity.</p> <p></p> </li> </ol>"},{"location":"AutoGPT/testing/","title":"Running tests","text":"<p>To run all tests, use the following command:</p> <pre><code>pytest\n</code></pre> <p>If <code>pytest</code> is not found:</p> <pre><code>python -m pytest\n</code></pre>"},{"location":"AutoGPT/testing/#running-specific-test-suites","title":"Running specific test suites","text":"<ul> <li>To run without integration tests:</li> </ul> <pre><code>pytest --without-integration\n</code></pre> <ul> <li>To run without slow integration tests:</li> </ul> <pre><code>pytest --without-slow-integration\n</code></pre> <ul> <li>To run tests and see coverage:</li> </ul> <pre><code>pytest --cov=autogpt --without-integration --without-slow-integration\n</code></pre>"},{"location":"AutoGPT/testing/#running-the-linter","title":"Running the linter","text":"<p>This project uses flake8 for linting. We currently use the following rules: <code>E303,W293,W291,W292,E305,E231,E302</code>. See the flake8 rules for more information.</p> <p>To run the linter:</p> <pre><code>flake8 .\n</code></pre> <p>Or:</p> <pre><code>python -m flake8 .\n</code></pre>"},{"location":"AutoGPT/usage/","title":"AutoGPT Agent User Guide","text":"<p>Note</p> <p>This guide assumes you are in the <code>autogpt</code> folder, where the AutoGPT Agent is located.</p>"},{"location":"AutoGPT/usage/#command-line-interface","title":"Command Line Interface","text":"<p>Running <code>./autogpt.sh</code> (or any of its subcommands) with <code>--help</code> lists all the possible sub-commands and arguments you can use:</p> <pre><code>$ ./autogpt.sh --help\nUsage: python -m autogpt [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  run    Sets up and runs an agent, based on the task specified by the...\n  serve  Starts an Agent Protocol compliant AutoGPT server, which creates...\n</code></pre> <p>For Windows users</p> <p>On Windows, use <code>.\\autogpt.bat</code> instead of <code>./autogpt.sh</code>. Everything else (subcommands, arguments) should work the same.</p> <p>Usage with Docker</p> <p>For use with Docker, replace the script in the examples with <code>docker compose run --rm auto-gpt</code>:</p> <pre><code>docker compose run --rm auto-gpt --ai-settings &lt;filename&gt;\ndocker compose run --rm auto-gpt serve\n</code></pre>"},{"location":"AutoGPT/usage/#run-cli-mode","title":"<code>run</code> \u2013 CLI mode","text":"<p>The <code>run</code> sub-command starts AutoGPT with the legacy CLI interface.</p> <code>./autogpt.sh run --help</code> <pre><code>$ ./autogpt.sh run --help\nUsage: python -m autogpt run [OPTIONS]\n\n  Sets up and runs an agent, based on the task specified by the user, or\n  resumes an existing agent.\n\nOptions:\n  -c, --continuous                Enable Continuous Mode\n  -y, --skip-reprompt             Skips the re-prompting messages at the\n                                  beginning of the script\n  -l, --continuous-limit INTEGER  Defines the number of times to run in\n                                  continuous mode\n  --speak                         Enable Speak Mode\n  --debug                         Enable Debug Mode\n  --gpt3only                      Enable GPT3.5 Only Mode\n  --gpt4only                      Enable GPT4 Only Mode\n  --skip-news                     Specifies whether to suppress the output of\n                                  latest news on startup.\n  --install-plugin-deps           Installs external dependencies for 3rd party\n                                  plugins.\n  --ai-name TEXT                  AI name override\n  --ai-role TEXT                  AI role override\n  --constraint TEXT               Add or override AI constraints to include in\n                                  the prompt; may be used multiple times to\n                                  pass multiple constraints\n  --resource TEXT                 Add or override AI resources to include in\n                                  the prompt; may be used multiple times to\n                                  pass multiple resources\n  --best-practice TEXT            Add or override AI best practices to include\n                                  in the prompt; may be used multiple times to\n                                  pass multiple best practices\n  --override-directives           If specified, --constraint, --resource and\n                                  --best-practice will override the AI's\n                                  directives instead of being appended to them\n  --component-config-file TEXT    Path to the json configuration file.\n  --help                          Show this message and exit.\n</code></pre> <p>This mode allows running a single agent, and saves the agent's state when terminated. This means you can resume agents at a later time. See also agent state.</p> <p>Note</p> <p>For legacy reasons, the CLI will default to the <code>run</code> subcommand when none is specified: running <code>./autogpt.sh run [OPTIONS]</code> does the same as <code>./autogpt.sh [OPTIONS]</code>, but this may change in the future.</p>"},{"location":"AutoGPT/usage/#continuous-mode","title":"\ud83d\udc80 Continuous Mode \u26a0\ufe0f","text":"<p>Run the AI without user authorization, 100% automated. Continuous mode is NOT recommended. It is potentially dangerous and may cause your AI to run forever or carry out actions you would not usually authorize. Use at your own risk.</p> <pre><code>./autogpt.sh --continuous\n</code></pre> <p>To exit the program, press Ctrl+C</p>"},{"location":"AutoGPT/usage/#serve-agent-protocol-mode-with-ui","title":"<code>serve</code> \u2013 Agent Protocol mode with UI","text":"<p>With <code>serve</code>, the application exposes an Agent Protocol compliant API and serves a frontend, by default on <code>http://localhost:8000</code>. You can configure the port it is served on with the <code>AP_SERVER_PORT</code> environment variable.</p> <code>./autogpt.sh serve --help</code> <pre><code>$ ./autogpt.sh serve --help\nUsage: python -m autogpt serve [OPTIONS]\n\n  Starts an Agent Protocol compliant AutoGPT server, which creates a custom\n  agent for every task.\n\nOptions:\n  --debug                     Enable Debug Mode\n  --gpt3only                  Enable GPT3.5 Only Mode\n  --gpt4only                  Enable GPT4 Only Mode\n  --install-plugin-deps       Installs external dependencies for 3rd party\n                              plugins.\n  --help                      Show this message and exit.\n</code></pre> <p>For more information about the API of the application, see agentprotocol.ai.</p>"},{"location":"AutoGPT/usage/#arguments","title":"Arguments","text":"<p>Attention</p> <p>Most arguments are equivalent to configuration options. See <code>.env.template</code> for all available configuration options.</p> <p>Note</p> <p>Replace anything in angled brackets (&lt;&gt;) to a value you want to specify</p> <p>Here are some common arguments you can use when running AutoGPT:</p> <ul> <li> <p>Run AutoGPT with a different AI Settings file</p> <pre><code>./autogpt.sh --ai-settings &lt;filename&gt;\n</code></pre> </li> <li> <p>Run AutoGPT with a different Prompt Settings file</p> <pre><code>./autogpt.sh --prompt-settings &lt;filename&gt;\n</code></pre> </li> </ul> <p>Note</p> <p>There are shorthands for some of these flags, for example <code>-P</code> for <code>--prompt-settings</code>. Use <code>./autogpt.sh --help</code> for more information.</p>"},{"location":"AutoGPT/usage/#agent-state","title":"Agent State","text":"<p>The state of individual agents is stored in the <code>data/agents</code> folder. You can use this in various ways:</p> <ul> <li>Resume your agent at a later time.</li> <li>Create \"checkpoints\" for your agent so you can always go back to specific points in     its history.</li> <li>Share your agent!</li> </ul>"},{"location":"AutoGPT/usage/#workspace","title":"Workspace","text":"<p>Agents can read and write files. This happens in the <code>workspace</code> folder, which is in <code>data/agents/&lt;agent_id&gt;/</code>. Files outside of this folder can not be accessed by the agent unless <code>RESTRICT_TO_WORKSPACE</code> is set to <code>False</code>.</p> <p>Warning</p> <p>We do not recommend disabling <code>RESTRICT_TO_WORKSPACE</code>, unless AutoGPT is running in a sandbox environment where it couldn't do any damage (e.g. Docker or a VM).</p>"},{"location":"AutoGPT/usage/#logs","title":"Logs","text":"<p>Activity, Error, and Debug logs are located in <code>logs</code>.</p> <p>Tip</p> <p>Do you notice weird behavior with your agent? Do you have an interesting use case? Do you have a bug you want to report? Follow the step below to enable your logs. You can include these logs when making an issue report or discussing an issue with us.</p> <p>To print out debug logs:</p> <pre><code>./autogpt.sh --debug\n</code></pre>"},{"location":"AutoGPT/usage/#disabling-commands","title":"Disabling Commands","text":"<p>The best way to disable commands is to disable or remove the component that provides them. However, if you want to selectively disable some commands, you can use the <code>DISABLED_COMMANDS</code> config in your <code>.env</code>. Put the names of the commands you want to disable, separated by commas. You can find the list of commands in built-in components here.</p> <p>For example, to disable python coding features, set it to the value below:</p> <pre><code>DISABLED_COMMANDS=execute_python_code,execute_python_file\n</code></pre>"},{"location":"AutoGPT/configuration/options/","title":"Configuration","text":"<p>Configuration of sensitive settings such as API credentials is done through environment variables. You can set configuration variables via the <code>.env</code> file. If you don't have a <code>.env</code> file, create a copy of <code>.env.template</code> in your <code>AutoGPT</code> folder and name it <code>.env</code>.</p>"},{"location":"AutoGPT/configuration/options/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>AUTHORISE_COMMAND_KEY</code>: Key response accepted when authorising commands. Default: y</li> <li><code>ANTHROPIC_API_KEY</code>: Set this if you want to use Anthropic models with AutoGPT</li> <li><code>AZURE_CONFIG_FILE</code>: Location of the Azure Config file relative to the AutoGPT root directory. Default: azure.yaml</li> <li><code>COMPONENT_CONFIG_FILE</code>: Path to the component configuration file (json) for an agent. Optional</li> <li><code>DISABLED_COMMANDS</code>: Commands to disable. Use comma separated names of commands. See the list of commands from built-in components here. Default: None</li> <li><code>ELEVENLABS_API_KEY</code>: ElevenLabs API Key. Optional.</li> <li><code>ELEVENLABS_VOICE_ID</code>: ElevenLabs Voice ID. Optional.</li> <li><code>EMBEDDING_MODEL</code>: LLM Model to use for embedding tasks. Default: <code>text-embedding-3-small</code></li> <li><code>EXIT_KEY</code>: Exit key accepted to exit. Default: n</li> <li><code>FAST_LLM</code>: LLM Model to use for most tasks. Default: <code>gpt-3.5-turbo-0125</code></li> <li><code>GITHUB_API_KEY</code>: Github API Key. Optional.</li> <li><code>GITHUB_USERNAME</code>: GitHub Username. Optional.</li> <li><code>GOOGLE_API_KEY</code>: Google API key. Optional.</li> <li><code>GOOGLE_CUSTOM_SEARCH_ENGINE_ID</code>: Google custom search engine ID. Optional.</li> <li><code>GROQ_API_KEY</code>: Set this if you want to use Groq models with AutoGPT</li> <li><code>HUGGINGFACE_API_TOKEN</code>: HuggingFace API, to be used for both image generation and audio to text. Optional.</li> <li><code>HUGGINGFACE_IMAGE_MODEL</code>: HuggingFace model to use for image generation. Default: CompVis/stable-diffusion-v1-4</li> <li><code>LLAMAFILE_API_BASE</code>: Llamafile API base URL. Default: <code>http://localhost:8080/v1</code></li> <li><code>OPENAI_API_KEY</code>: Set this if you want to use OpenAI models; OpenAI API Key.</li> <li><code>OPENAI_ORGANIZATION</code>: Organization ID in OpenAI. Optional.</li> <li><code>PLAIN_OUTPUT</code>: Plain output, which disables the spinner. Default: False</li> <li><code>RESTRICT_TO_WORKSPACE</code>: The restrict file reading and writing to the workspace directory. Default: True</li> <li><code>SD_WEBUI_AUTH</code>: Stable Diffusion Web UI username:password pair. Optional.</li> <li><code>SMART_LLM</code>: LLM Model to use for \"smart\" tasks. Default: <code>gpt-4-turbo-preview</code></li> <li><code>STREAMELEMENTS_VOICE</code>: StreamElements voice to use. Default: Brian</li> <li><code>TEMPERATURE</code>: Value of temperature given to OpenAI. Value from 0 to 2. Lower is more deterministic, higher is more random. See https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature</li> <li><code>TEXT_TO_SPEECH_PROVIDER</code>: Text to Speech Provider. Options are <code>gtts</code>, <code>macos</code>, <code>elevenlabs</code>, and <code>streamelements</code>. Default: gtts</li> <li><code>USE_AZURE</code>: Use Azure's LLM Default: False</li> </ul>"},{"location":"AutoGPT/configuration/search/","title":"Search","text":""},{"location":"AutoGPT/configuration/search/#google-api-keys-configuration","title":"\ud83d\udd0d Google API Keys Configuration","text":"<p>Note</p> <p>This section is optional. Use the official Google API if search attempts return error 429. To use the <code>google</code> command, you need to set up your Google API key in your environment variables or pass it with configuration to the <code>WebSearchComponent</code>.</p> <p>Create your project:</p> <ol> <li>Go to the Google Cloud Console.</li> <li>If you don't already have an account, create one and log in</li> <li>Create a new project by clicking on the Select a Project dropdown at the top of the     page and clicking New Project</li> <li>Give it a name and click Create</li> <li>Set up a custom search API and add to your .env file:<ol> <li>Go to the APIs &amp; Services Dashboard</li> <li>Click Enable APIs and Services</li> <li>Search for Custom Search API and click on it</li> <li>Click Enable</li> <li>Go to the Credentials page</li> <li>Click Create Credentials</li> <li>Choose API Key</li> <li>Copy the API key</li> <li>Set it as the <code>GOOGLE_API_KEY</code> in your <code>.env</code> file</li> </ol> </li> <li>Enable     the Custom Search API on your project. (Might need to wait few minutes to propagate.)     Set up a custom search engine and add to your .env file:<ol> <li>Go to the Custom Search Engine page</li> <li>Click Add</li> <li>Set up your search engine by following the prompts.     You can choose to search the entire web or specific sites</li> <li>Once you've created your search engine, click on Control Panel</li> <li>Click Basics</li> <li>Copy the Search engine ID</li> <li>Set it as the <code>CUSTOM_SEARCH_ENGINE_ID</code> in your <code>.env</code> file</li> </ol> </li> </ol> <p>Remember that your free daily custom search quota allows only up to 100 searches. To increase this limit, you need to assign a billing account to the project to profit from up to 10K daily searches.</p>"},{"location":"AutoGPT/configuration/voice/","title":"Text to Speech","text":"<p>Enter this command to use TTS (Text-to-Speech) for AutoGPT</p> <pre><code>./autogpt.sh --speak\n</code></pre> <p>Eleven Labs provides voice technologies such as voice design, speech synthesis, and premade voices that AutoGPT can use for speech.</p> <ol> <li>Go to ElevenLabs and make an account if you don't     already have one.</li> <li>Choose and setup the Starter plan.</li> <li>Click the top right icon and find Profile to locate your API Key.</li> </ol> <p>In the <code>.env</code> file set:</p> <ul> <li><code>ELEVENLABS_API_KEY</code></li> <li><code>ELEVENLABS_VOICE_1_ID</code> (example: \"premade/Adam\")</li> </ul>"},{"location":"AutoGPT/configuration/voice/#list-of-available-voices","title":"List of available voices","text":"<p>Note</p> <p>You can use either the name or the voice ID to configure a voice</p> Name Voice ID Rachel <code>21m00Tcm4TlvDq8ikWAM</code> Domi <code>AZnzlk1XvdvUeBnXmlld</code> Bella <code>EXAVITQu4vr4xnSDxMaL</code> Antoni <code>ErXwobaYiN019PkySvjV</code> Elli <code>MF3mGyEYCl7XYWbV9V6O</code> Josh <code>TxGEqnHWrfWFTfGW9XjX</code> Arnold <code>VR6AewLTigWG4xSOukaG</code> Adam <code>pNInz6obpgDQGcFmaJgB</code> Sam <code>yoZ06aMxZJJ28mfd3POQ</code>"},{"location":"AutoGPT/setup/","title":"AutoGPT Agent setup","text":"<p>\ud83d\udc0b Set up &amp; Run with Docker \u2002|\u2002 \ud83d\udc77\ud83c\udffc For Developers</p>"},{"location":"AutoGPT/setup/#requirements","title":"\ud83d\udccb Requirements","text":""},{"location":"AutoGPT/setup/#linux-macos","title":"Linux / macOS","text":"<ul> <li>Python 3.10 or later</li> <li>Poetry (instructions)</li> </ul>"},{"location":"AutoGPT/setup/#windows-wsl","title":"Windows (WSL)","text":"<ul> <li>WSL 2</li> <li>See also the requirements for Linux</li> <li>Docker Desktop</li> </ul>"},{"location":"AutoGPT/setup/#windows","title":"Windows","text":"<p>Attention</p> <p>We recommend setting up AutoGPT with WSL. Some things don't work exactly the same on Windows and we currently can't provide specialized instructions for all those cases.</p> <ul> <li>Python 3.10 or later (instructions)</li> <li>Poetry (instructions)</li> <li>Docker Desktop</li> </ul>"},{"location":"AutoGPT/setup/#setting-up-autogpt","title":"Setting up AutoGPT","text":""},{"location":"AutoGPT/setup/#getting-autogpt","title":"Getting AutoGPT","text":"<p>Since we don't ship AutoGPT as a desktop application, you'll need to download the project from GitHub and give it a place on your computer.</p> <p></p> <ul> <li>To get the latest bleeding edge version, use <code>master</code>.</li> <li>If you're looking for more stability, check out the latest AutoGPT release.</li> </ul> <p>Note</p> <p>These instructions don't apply if you're looking to run AutoGPT as a docker image. Instead, check out the Docker setup guide.</p>"},{"location":"AutoGPT/setup/#completing-the-setup","title":"Completing the Setup","text":"<p>Once you have cloned or downloaded the project, you can find the AutoGPT Agent in the <code>autogpt/</code> folder. Inside this folder you can configure the AutoGPT application with an <code>.env</code> file and (optionally) a JSON configuration file:</p> <ul> <li><code>.env</code> for environment variables, which are mostly used for sensitive data like API keys</li> <li>a JSON configuration file to customize certain features of AutoGPT's Components</li> </ul> <p>See the Configuration reference for a list of available environment variables.</p> <ol> <li>Find the file named <code>.env.template</code>. This file may     be hidden by default in some operating systems due to the dot prefix. To reveal     hidden files, follow the instructions for your specific operating system:     Windows and macOS.</li> <li>Create a copy of <code>.env.template</code> and call it <code>.env</code>;     if you're already in a command prompt/terminal window:     <pre><code>cp .env.template .env\n</code></pre></li> <li>Open the <code>.env</code> file in a text editor.</li> <li>Set API keys for the LLM providers that you want to use: see below.</li> <li> <p>Enter any other API keys or tokens for services you would like to use.</p> <p>Note</p> <p>To activate and adjust a setting, remove the <code>#</code> prefix.</p> </li> <li> <p>Save and close the <code>.env</code> file.</p> </li> <li>Optional: run <code>poetry install</code> to install all required dependencies. The     application also checks for and installs any required dependencies when it starts.</li> <li>Optional: configure the JSON file (e.g. <code>config.json</code>) with your desired settings.     The application will use default settings if you don't provide a JSON configuration file.     Learn how to set up the JSON configuration file</li> </ol> <p>You should now be able to explore the CLI (<code>./autogpt.sh --help</code>) and run the application.</p> <p>See the user guide for further instructions.</p>"},{"location":"AutoGPT/setup/#setting-up-llm-providers","title":"Setting up LLM providers","text":"<p>You can use AutoGPT with any of the following LLM providers. Each of them comes with its own setup instructions.</p> <p>AutoGPT was originally built on top of OpenAI's GPT-4, but now you can get similar and interesting results using other models/providers too. If you don't know which to choose, you can safely go with OpenAI*.</p> <p>* subject to change</p>"},{"location":"AutoGPT/setup/#openai","title":"OpenAI","text":"<p>Attention</p> <p>To use AutoGPT with GPT-4 (recommended), you need to set up a paid OpenAI account with some money in it. Please refer to OpenAI for further instructions (link). Free accounts are limited to GPT-3.5 with only 3 requests per minute.</p> <ol> <li>Make sure you have a paid account with some credits set up: Settings &gt; Organization &gt; Billing</li> <li>Get your OpenAI API key from: API keys</li> <li>Open <code>.env</code></li> <li>Find the line that says <code>OPENAI_API_KEY=</code></li> <li> <p>Insert your OpenAI API Key directly after = without quotes or spaces:     <pre><code>OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre></p> <p>Using a GPT Azure-instance</p> <p>If you want to use GPT on an Azure instance, set <code>USE_AZURE</code> to <code>True</code> and make an Azure configuration file.</p> <p>Rename <code>azure.yaml.template</code> to <code>azure.yaml</code> and provide the relevant <code>azure_api_base</code>, <code>azure_api_version</code> and deployment IDs for the models that you want to use.</p> <p>E.g. if you want to use <code>gpt-3.5-turbo</code> and <code>gpt-4-turbo</code>:</p> <pre><code># Please specify all of these values as double-quoted strings\n# Replace string in angled brackets (&lt;&gt;) to your own deployment Name\nazure_model_map:\n    gpt-3.5-turbo: \"&lt;gpt-35-turbo-deployment-id&gt;\"\n    gpt-4-turbo: \"&lt;gpt-4-turbo-deployment-id&gt;\"\n    ...\n</code></pre> <p>Details can be found in the openai/python-sdk/azure, and in the [Azure OpenAI docs] for the embedding model. If you're on Windows you may need to install an MSVC library.</p> </li> </ol> <p>Important</p> <p>Keep an eye on your API costs on the Usage page.</p>"},{"location":"AutoGPT/setup/#anthropic","title":"Anthropic","text":"<ol> <li>Make sure you have credits in your account: Settings &gt; Plans &amp; billing</li> <li>Get your Anthropic API key from Settings &gt; API keys</li> <li>Open <code>.env</code></li> <li>Find the line that says <code>ANTHROPIC_API_KEY=</code></li> <li>Insert your Anthropic API Key directly after = without quotes or spaces:     <pre><code>ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre></li> <li>Set <code>SMART_LLM</code> and/or <code>FAST_LLM</code> to the Claude 3 model you want to use.    See Anthropic's models overview for info on the available models.    Example:     <pre><code>SMART_LLM=claude-3-opus-20240229\n</code></pre></li> </ol> <p>Important</p> <p>Keep an eye on your API costs on the Usage page.</p>"},{"location":"AutoGPT/setup/#groq","title":"Groq","text":"<p>Note</p> <p>Although Groq is supported, its built-in function calling API isn't mature. Any features using this API may experience degraded performance. Let us know your experience!</p> <ol> <li>Get your Groq API key from Settings &gt; API keys</li> <li>Open <code>.env</code></li> <li>Find the line that says <code>GROQ_API_KEY=</code></li> <li>Insert your Anthropic API Key directly after = without quotes or spaces:     <pre><code>GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n</code></pre></li> <li>Set <code>SMART_LLM</code> and/or <code>FAST_LLM</code> to the Groq model you want to use.    See Groq's models overview for info on the available models.    Example:     <pre><code>SMART_LLM=llama3-70b-8192\n</code></pre></li> </ol>"},{"location":"AutoGPT/setup/#llamafile","title":"Llamafile","text":"<p>With llamafile you can run models locally, which means no need to set up billing, and guaranteed data privacy.</p> <p>For more information and in-depth documentation, check out the llamafile documentation.</p> <p>Warning</p> <p>At the moment, llamafile only serves one model at a time. This means you can not set <code>SMART_LLM</code> and <code>FAST_LLM</code> to two different llamafile models.</p> <p>Warning</p> <p>Due to the issues linked below, llamafiles don't work on WSL. To use a llamafile with AutoGPT in WSL, you will have to run the llamafile in Windows (outside WSL).</p> <p> Instructions <ol> <li>Get the <code>llamafile/serve.py</code> script through one of these two ways:<ol> <li>Clone the AutoGPT repo somewhere in your Windows environment,    with the script located at <code>autogpt/scripts/llamafile/serve.py</code></li> <li>Download just the serve.py script somewhere in your Windows environment</li> </ol> </li> <li>Make sure you have <code>click</code> installed: <code>pip install click</code></li> <li>Run <code>ip route | grep default | awk '{print $3}'</code> inside WSL to get the address    of the WSL host machine</li> <li>Run <code>python3 serve.py --host {WSL_HOST_ADDR}</code>, where <code>{WSL_HOST_ADDR}</code>    is the address you found at step 3.    If port 8080 is taken, also specify a different port using <code>--port {PORT}</code>.</li> <li>In WSL, set <code>LLAMAFILE_API_BASE=http://{WSL_HOST_ADDR}:8080/v1</code> in your <code>.env</code>.</li> <li>Follow the rest of the regular instructions below.</li> </ol> <ul> <li>Mozilla-Ocho/llamafile#356</li> <li>Mozilla-Ocho/llamafile#100</li> </ul> <p>Note</p> <p>These instructions will download and use <code>mistral-7b-instruct-v0.2.Q5_K_M.llamafile</code>. <code>mistral-7b-instruct-v0.2</code> is currently the only tested and supported model. If you want to try other models, you'll have to add them to <code>LlamafileModelName</code> in <code>llamafile.py</code>. For optimal results, you may also have to add some logic to adapt the message format, like <code>LlamafileProvider._adapt_chat_messages_for_mistral_instruct(..)</code> does.</p> <ol> <li>Run the llamafile serve script:    <pre><code>python3 ./scripts/llamafile/serve.py\n</code></pre>    The first time this is run, it will download a file containing the model + runtime,    which may take a while and a few gigabytes of disk space.</li> </ol> <p>To force GPU acceleration, add <code>--use-gpu</code> to the command.</p> <ol> <li> <p>In <code>.env</code>, set <code>SMART_LLM</code>/<code>FAST_LLM</code> or both to <code>mistral-7b-instruct-v0.2</code></p> </li> <li> <p>If the server is running on different address than <code>http://localhost:8080/v1</code>,    set <code>LLAMAFILE_API_BASE</code> in <code>.env</code> to the right base URL</p> </li> </ol>"},{"location":"AutoGPT/setup/docker/","title":"AutoGPT + Docker guide","text":"<p>Important</p> <p>Docker Compose version 1.29.0 or later is required to use version 3.9 of the Compose file format. You can check the version of Docker Compose installed on your system by running the following command:</p> <pre><code>docker compose version\n</code></pre> <p>This will display the version of Docker Compose that is currently installed on your system.</p> <p>If you need to upgrade Docker Compose to a newer version, you can follow the installation instructions in the Docker documentation: https://docs.docker.com/compose/install/</p>"},{"location":"AutoGPT/setup/docker/#basic-setup","title":"Basic Setup","text":"<ol> <li>Make sure you have Docker installed, see requirements</li> <li> <p>Create a project directory for AutoGPT</p> <pre><code>mkdir AutoGPT\ncd AutoGPT\n</code></pre> </li> <li> <p>In the project directory, create a file called <code>docker-compose.yml</code>:</p> <p> <code>docker-compose.yml&gt;</code> for &lt;= v0.4.7  <p><pre><code>version: \"3.9\"\nservices:\n  auto-gpt:\n    image: significantgravitas/auto-gpt\n    env_file:\n      - .env\n    profiles: [\"exclude-from-up\"]\n    volumes:\n      - ./auto_gpt_workspace:/app/auto_gpt_workspace\n      - ./data:/app/data\n      ## allow auto-gpt to write logs to disk\n      - ./logs:/app/logs\n      ## uncomment following lines if you want to make use of these files\n      ## you must have them existing in the same folder as this docker-compose.yml\n      #- type: bind\n      #  source: ./azure.yaml\n      #  target: /app/azure.yaml\n</code></pre> </p> <p> <code>docker-compose.yml&gt;</code> for &gt; v0.4.7 (including <code>master</code>)  <p><pre><code>version: \"3.9\"\nservices:\n  auto-gpt:\n    image: significantgravitas/auto-gpt\n    env_file:\n      - .env\n    ports:\n      - \"8000:8000\"  # remove this if you just want to run a single agent in TTY mode\n    profiles: [\"exclude-from-up\"]\n    volumes:\n      - ./data:/app/data\n      ## allow auto-gpt to write logs to disk\n      - ./logs:/app/logs\n      ## uncomment following lines if you want to make use of these files\n      ## you must have them existing in the same folder as this docker-compose.yml\n      ## component configuration file\n      #- type: bind\n      #  source: ./config.json\n      #  target: /app/config.json\n</code></pre> </p> <li> <p>Download <code>.env.template</code> and save it as <code>.env</code> in the AutoGPT folder.</p> </li> <li>Follow the standard configuration instructions,    from step 3 onwards and excluding <code>poetry install</code> steps.</li> <li> <p>Pull the latest image from Docker Hub</p> <p><pre><code>docker pull significantgravitas/auto-gpt\n</code></pre> 4. Optional: mount configuration file.   If you have component configuration file, for example <code>config.json</code>, place it in <code>autogpt/data/</code> directory. Or place it in <code>autogpt/</code> and uncomment the line in <code>docker-compose.yml</code> that mounts it.   To learn more about configuring, see Component configuration</p> </li> <p>Docker only supports headless browsing</p> <p>AutoGPT uses a browser in headless mode by default: <code>HEADLESS_BROWSER=True</code>. Please do not change this setting in combination with Docker, or AutoGPT will crash.</p>"},{"location":"AutoGPT/setup/docker/#developer-setup","title":"Developer Setup","text":"<p>Tip</p> <p>Use this setup if you have cloned the repository and have made (or want to make) changes to the codebase.</p> <ol> <li>Copy <code>.env.template</code> to <code>.env</code>.</li> <li>Follow the standard configuration instructions,    from step 3 onwards and excluding <code>poetry install</code> steps.</li> </ol>"},{"location":"AutoGPT/setup/docker/#running-autogpt-with-docker","title":"Running AutoGPT with Docker","text":"<p>After following setup instructions above, you can run AutoGPT with the following command:</p> <pre><code>docker compose run --rm auto-gpt\n</code></pre> <p>This creates and starts an AutoGPT container, and removes it after the application stops. This does not mean your data will be lost: data generated by the application is stored in the <code>data</code> folder.</p> <p>Subcommands and arguments work the same as described in the user guide:</p> <ul> <li>Run AutoGPT:     <pre><code>docker compose run --rm auto-gpt serve\n</code></pre></li> <li>Run AutoGPT in TTY mode, with continuous mode.     <pre><code>docker compose run --rm auto-gpt run --continuous\n</code></pre></li> <li>Run AutoGPT in TTY mode and install dependencies for all active plugins:     <pre><code>docker compose run --rm auto-gpt run --install-plugin-deps\n</code></pre></li> </ul> <p>If you dare, you can also build and run it with \"vanilla\" docker commands:</p> <pre><code>docker build -t autogpt .\ndocker run -it --env-file=.env -v $PWD:/app autogpt\ndocker run -it --env-file=.env -v $PWD:/app --rm autogpt --gpt3only --continuous\n</code></pre>"},{"location":"AutoGPT/setup/for-developers/","title":"For Developers","text":"<p>Coming soon...</p> <p>TODO: - Running and developing AutoGPT with VSCode + devcontainer - Resources     - Prompt engineering guides     - Related projects     - Papers - Technical documentation</p>"},{"location":"challenges/beat/","title":"Beat a Challenge","text":"<p>If you have a solution or idea to tackle an existing challenge, you can contribute by working on it and submitting your solution. Here's how to get started:</p>"},{"location":"challenges/beat/#guidelines-for-beating-a-challenge","title":"Guidelines for Beating a Challenge","text":"<ol> <li> <p>Choose a challenge: Browse the List of Challenges and choose one that interests you or aligns with your expertise.</p> </li> <li> <p>Understand the problem: Make sure you thoroughly understand the problem at hand, its scope, and the desired outcome.</p> </li> <li> <p>Develop a solution: Work on creating a solution for the challenge. This may/</p> </li> </ol>"},{"location":"challenges/building_challenges/","title":"Creating Challenges for AutoGPT","text":"<p>\ud83c\udff9 We're on the hunt for talented Challenge Creators! \ud83c\udfaf</p> <p>Join us in shaping the future of AutoGPT by designing challenges that test its limits. Your input will be invaluable in guiding our progress and ensuring that we're on the right track. We're seeking individuals with a diverse skill set, including:</p> <p>\ud83c\udfa8 UX Design: Your expertise will enhance the user experience for those attempting to conquer our challenges. With your help, we'll develop a dedicated section in our wiki, and potentially even launch a standalone website.</p> <p>\ud83d\udcbb Coding Skills: Proficiency in Python, pytest, and VCR (a library that records OpenAI calls and stores them) will be essential for creating engaging and robust challenges.</p> <p>\u2699\ufe0f DevOps Skills: Experience with CI pipelines in GitHub and possibly Google Cloud Platform will be instrumental in streamlining our operations.</p> <p>Are you ready to play a pivotal role in AutoGPT's journey? Apply now to become a Challenge Creator by opening a PR! \ud83d\ude80</p>"},{"location":"challenges/building_challenges/#getting-started","title":"Getting Started","text":"<p>Clone the original AutoGPT repo and checkout to master branch</p> <p>The challenges are not written using a specific framework. They try to be very agnostic The challenges are acting like a user that wants something done:  INPUT: - User desire - Files, other inputs</p> <p>Output =&gt; Artifact (files, image, code, etc, etc...)</p>"},{"location":"challenges/building_challenges/#defining-your-agent","title":"Defining your Agent","text":"<p>Go to https://github.com/Significant-Gravitas/AutoGPT/blob/master/autogpt/tests/integration/agent_factory.py</p> <p>Create your agent fixture.</p> <pre><code>def kubernetes_agent(\n    agent_test_config, workspace: Workspace\n):\n    # Please choose the commands your agent will need to beat the challenges, the full list is available in the main.py\n    # (we 're working on a better way to design this, for now you have to look at main.py)\n    command_registry = CommandRegistry()\n    command_registry.import_commands(\"autogpt.commands.file_operations\")\n    command_registry.import_commands(\"autogpt.app\")\n\n    # Define all the settings of our challenged agent\n    ai_profile = AIProfile(\n        ai_name=\"Kubernetes\",\n        ai_role=\"an autonomous agent that specializes in creating Kubernetes deployment templates.\",\n        ai_goals=[\n            \"Write a simple kubernetes deployment file and save it as a kube.yaml.\",\n        ],\n    )\n    ai_profile.command_registry = command_registry\n\n    system_prompt = ai_profile.construct_full_prompt()\n    agent_test_config.set_continuous_mode(False)\n    agent = Agent(\n        command_registry=command_registry,\n        config=ai_profile,\n        next_action_count=0,\n        triggering_prompt=DEFAULT_TRIGGERING_PROMPT,\n    )\n\n    return agent\n</code></pre>"},{"location":"challenges/building_challenges/#creating-your-challenge","title":"Creating your challenge","text":"<p>Go to <code>tests/challenges</code>and create a file that is called <code>test_your_test_description.py</code> and add it to the appropriate folder. If no category exists you can create a new one.</p> <p>Your test could look something like this </p> <pre><code>import contextlib\nfrom functools import wraps\nfrom typing import Generator\n\nimport pytest\nimport yaml\n\nfrom autogpt.commands.file_operations import read_file, write_to_file\nfrom tests.integration.agent_utils import run_interaction_loop\nfrom tests.challenges.utils import run_multiple_times\n\ndef input_generator(input_sequence: list) -&gt; Generator[str, None, None]:\n    \"\"\"\n    Creates a generator that yields input strings from the given sequence.\n\n    :param input_sequence: A list of input strings.\n    :return: A generator that yields input strings.\n    \"\"\"\n    yield from input_sequence\n\n\n@pytest.mark.skip(\"This challenge hasn't been beaten yet.\")\n@pytest.mark.vcr\n@pytest.mark.requires_openai_api_key\ndef test_information_retrieval_challenge_a(kubernetes_agent, monkeypatch) -&gt; None:\n    \"\"\"\n    Test the challenge_a function in a given agent by mocking user inputs\n    and checking the output file content.\n\n    :param get_company_revenue_agent: The agent to test.\n    :param monkeypatch: pytest's monkeypatch utility for modifying builtins.\n    \"\"\"\n    input_sequence = [\"s\", \"s\", \"s\", \"s\", \"s\", \"EXIT\"]\n    gen = input_generator(input_sequence)\n    monkeypatch.setattr(\"autogpt.utils.session.prompt\", lambda _: next(gen))\n\n    with contextlib.suppress(SystemExit):\n        run_interaction_loop(kubernetes_agent, None)\n\n    # here we load the output file\n    file_path = str(kubernetes_agent.workspace.get_path(\"kube.yaml\"))\n    content = read_file(file_path)\n\n    # then we check if it's including keywords from the kubernetes deployment config\n    for word in [\"apiVersion\", \"kind\", \"metadata\", \"spec\"]:\n        assert word in content, f\"Expected the file to contain {word}\"\n\n    content = yaml.safe_load(content)\n    for word in [\"Service\", \"Deployment\", \"Pod\"]:\n        assert word in content[\"kind\"], f\"Expected the file to contain {word}\"\n</code></pre>"},{"location":"challenges/challenge_template/","title":"Challenge Title","text":""},{"location":"challenges/challenge_template/#description","title":"Description","text":"<p>Provide a clear and concise description of the challenge. Include any relevant examples or files to illustrate the problem.</p>"},{"location":"challenges/challenge_template/#input","title":"Input","text":"<p>If the challenge involves specific input files, describe them here. Provide the file names and their contents, if necessary. Use triple backticks (```) to format the content as a code block.</p> <p>For example:</p> <p>instructions_1.txt</p> <p>The current task_id is 4563.\\n[NOISE intended to confuse the agent] Read the file instructions_2.txt using the read_file command.</p>"},{"location":"challenges/challenge_template/#scope","title":"Scope","text":"<p>Define the scope of the challenge, including any relevant constraints, requirements, or limitations.</p>"},{"location":"challenges/challenge_template/#success-evaluation","title":"Success Evaluation","text":"<p>Explain how success will be measured or evaluated for the challenge. This helps others understand what the desired outcome is and how to work towards it.</p>"},{"location":"challenges/introduction/","title":"Introduction to Challenges","text":"<p>Welcome to the AutoGPT Challenges page! This is a space where we encourage community members to collaborate and contribute towards improving AutoGPT by identifying and solving challenges that AutoGPT is not yet able to achieve.</p>"},{"location":"challenges/introduction/#what-are-challenges","title":"What are challenges?","text":"<p>Challenges are tasks or problems that AutoGPT has difficulty solving or has not yet been able to accomplish. These may include improving specific functionalities, enhancing the model's understanding of specific domains, or even developing new features that the current version of AutoGPT lacks.</p>"},{"location":"challenges/introduction/#why-are-challenges-important","title":"Why are challenges important?","text":"<p>Addressing challenges helps us improve AutoGPT's performance, usability, and versatility. By working together to tackle these challenges, we can create a more powerful and efficient tool for everyone. It also allows the community to actively contribute to the project, making it a true open-source effort.</p>"},{"location":"challenges/introduction/#how-can-you-participate","title":"How can you participate?","text":"<p>There are two main ways to get involved with challenges:</p> <ol> <li>Submit a Challenge: If you have identified a task that AutoGPT struggles with, you can submit it as a challenge. This allows others to see the issue and collaborate on finding a solution.</li> <li>Beat a Challenge: If you have a solution or idea to tackle an existing challenge, you can contribute by working on the challenge and submitting your solution.</li> </ol> <p>To learn more about submitting and beating challenges, please visit the List of Challenges, Submit a Challenge, and Beat a Challenge pages.</p> <p>We look forward to your contributions and the exciting solutions that the community will develop together to make AutoGPT even better!</p> <p>Warning</p> <p>We're slowly transitioning to agbenchmark. agbenchmark is a simpler way to improve AutoGPT. Simply run:</p> <pre><code>agbenchmark\n</code></pre> <p>and beat as many challenges as possible.</p> <p>For more agbenchmark options, look at the readme.</p>"},{"location":"challenges/list/","title":"List of Challenges","text":"<p>This page contains a curated list of challenges that AutoGPT currently faces. If you think you have a solution or idea to tackle any of these challenges, feel free to dive in and start working on them! New challenges can also be submitted by following the guidelines on the Submit a Challenge page.</p> <p>Memory Challenges: List of Challenges</p>"},{"location":"challenges/submit/","title":"Submit a Challenge","text":"<p>If you have identified a task or problem that AutoGPT struggles with, you can submit it as a challenge for the community to tackle. Here's how you can submit a new challenge:</p>"},{"location":"challenges/submit/#how-to-submit-a-challenge","title":"How to Submit a Challenge","text":"<ol> <li>Create a new <code>.md</code> file in the <code>challenges</code> directory in the AutoGPT GitHub repository. Make sure to pick the right category. </li> <li>Name the file with a descriptive title for the challenge, using hyphens instead of spaces (e.g., <code>improve-context-understanding.md</code>).</li> <li>In the file, follow the challenge_template.md to describe the problem, define the scope, and evaluate success.</li> <li>Commit the file and create a pull request.</li> </ol> <p>Once submitted, the community can review and discuss the challenge. If deemed appropriate, it will be added to the List of Challenges.</p> <p>If you're looking to contribute by working on an existing challenge, check out Beat a Challenge for guidelines on how to get started.</p>"},{"location":"challenges/information_retrieval/challenge_a/","title":"Information Retrieval Challenge A","text":"<p>Status: Current level to beat: level 2</p> <p>Command to try:</p> <pre><code>pytest -s tests/challenges/information_retrieval/test_information_retrieval_challenge_a.py --level=2\n</code></pre>"},{"location":"challenges/information_retrieval/challenge_a/#description","title":"Description","text":"<p>The agent's goal is to find the revenue of Tesla: - level 1 asks the revenue of Tesla in 2022 and explicitly asks to search for 'tesla revenue 2022' - level 2 is identical but doesn't ask to search for 'tesla revenue 2022' - level 3 asks for tesla's revenue by year since its creation.</p> <p>It should write the result in a file called output.txt.</p> <p>The agent should be able to beat this test consistently (this is the hardest part).</p>"},{"location":"challenges/information_retrieval/challenge_a/#objective","title":"Objective","text":"<p>The objective of this challenge is to test the agent's ability to retrieve information in a consistent way.</p>"},{"location":"challenges/information_retrieval/challenge_b/","title":"Information Retrieval Challenge B","text":"<p>Status: Beaten</p> <p>Command to try:</p> <pre><code>pytest -s tests/challenges/information_retrieval/test_information_retrieval_challenge_b.py\n</code></pre>"},{"location":"challenges/information_retrieval/challenge_b/#description","title":"Description","text":"<p>The agent's goal is to find the names, affiliated university, and discovery of the individuals who won the nobel prize for physics in 2010.</p> <p>It should write the result in a file called 2010_nobel_prize_winners.txt.</p> <p>The agent should be able to beat this test consistently (this is the hardest part).</p>"},{"location":"challenges/information_retrieval/challenge_b/#objective","title":"Objective","text":"<p>The objective of this challenge is to test the agent's ability to retrieve multiple pieces of related information in a consistent way. The agent should not use google to perform the task, because it should already know the answer. This why the task fails after 2 cycles (1 cycle to retrieve information, 1 cycle to write the file)</p>"},{"location":"challenges/information_retrieval/introduction/","title":"Information Retrieval","text":"<p>Information retrieval challenges are designed to evaluate the proficiency of an AI agent, such as AutoGPT, in searching, extracting, and presenting relevant information from a vast array of sources. These challenges often encompass tasks such as interpreting user queries, browsing the web, and filtering through unstructured data.</p>"},{"location":"challenges/memory/challenge_a/","title":"Memory Challenge A","text":"<p>Status: Challenge Completed</p>"},{"location":"challenges/memory/challenge_a/#description","title":"Description","text":"<p>The agent, Follow-Instructions-GPT, has the following goals:</p> <ul> <li>Use the command <code>read_file</code> to read the <code>instructions_1.txt</code> file.</li> <li>Follow the instructions in the <code>instructions_1.txt</code> file.</li> </ul> <p>The challenge involves a series of text files containing instructions. The agent must follow the instructions and keep track of the task ID as it progresses through the files.</p>"},{"location":"challenges/memory/challenge_a/#files","title":"Files","text":"<ul> <li><code>instructions_1.txt</code></li> </ul> <p>The task_id is 1234 Read the file instructions_1.txt</p> <ul> <li><code>instructions_2.txt</code></li> </ul> <p>Read the file instructions_2.txt using the read_file command.</p> <p>...and so on.</p> <ul> <li><code>instructions_n.txt</code></li> </ul> <p>Write the task_id into the file output.txt.</p>"},{"location":"challenges/memory/challenge_a/#objective","title":"Objective","text":"<p>The objective of this challenge is to test the agent's ability to follow instructions and maintain memory of the task Id throughout the process. The agent successfully completed this challenge if it wrote the task id in a file.</p>"},{"location":"challenges/memory/challenge_b/","title":"Memory Challenge B","text":"<p>Status: Current level to beat: level 3</p> <p>Command to try: </p> <pre><code>pytest -s tests/challenges/memory/test_memory_challenge_b.py --level=3\n</code></pre>"},{"location":"challenges/memory/challenge_b/#description","title":"Description","text":"<p>The agent, Follow-Instructions-GPT, has the following goals:</p> <ul> <li>Use the command <code>read_file</code> to read the <code>instructions_1.txt</code> file.</li> <li>Follow the instructions in the <code>instructions_1.txt</code> file.</li> </ul> <p>The challenge involves a series of text files containing instructions and task IDs. The agent must follow the instructions and keep track of the task IDs as it progresses through the files.</p>"},{"location":"challenges/memory/challenge_b/#files","title":"Files","text":"<ul> <li><code>instructions_1.txt</code></li> </ul> <p>The current task_id is 4563.\\n[NOISE intended to confuse the agent] Read the file instructions_2.txt using the read_file command.</p> <ul> <li><code>instructions_2.txt</code></li> </ul> <p>The current task_id is 6182.\\n[NOISE intended to confuse the agent] Read the file instructions_3.txt using the read_file command.</p> <p>...and so on.</p> <ul> <li><code>instructions_n.txt</code></li> </ul> <p>The current task_id is 8912. Write all the task_ids into the file output.txt. The file has not been created yet. After that, use the task_complete command.</p>"},{"location":"challenges/memory/challenge_b/#objective","title":"Objective","text":"<p>The objective of this challenge is to test the agent's ability to follow instructions and maintain memory of the task IDs throughout the process. The agent successfully completed this challenge if it wrote the task ids in a file.</p>"},{"location":"challenges/memory/challenge_c/","title":"Memory Challenge C","text":"<p>Status: Current level to beat: level 1</p> <p>Command to try: </p> <pre><code>pytest -s tests/challenges/memory/test_memory_challenge_c.py --level=2\n</code></pre>"},{"location":"challenges/memory/challenge_c/#description","title":"Description","text":"<p>The agent, Follow-Instructions-GPT, has the following goals:</p> <ul> <li>Use the command <code>read_file</code> to read the <code>instructions_1.txt</code> file.</li> <li>Follow the instructions in the <code>instructions_1.txt</code> file.</li> </ul> <p>The challenge involves a series of text files containing instructions and silly phrases. The agent must follow the instructions and keep track of the task IDs as it progresses through the files.</p>"},{"location":"challenges/memory/challenge_c/#files","title":"Files","text":"<ul> <li><code>instructions_1.txt</code></li> </ul> <p>The current phrase is </p> <pre><code>The purple elephant danced on a rainbow while eating a taco.\\n[NOISE intended to confuse the agent]\n</code></pre> <p>Read the file <code>instructions_2.txt</code> using the read_file command.</p> <ul> <li><code>instructions_2.txt</code></li> </ul> <p>The current phrase is </p> <pre><code>The sneaky toaster stole my socks and ran away to Hawaii.\\n[NOISE intended to confuse the agent]\n</code></pre> <p>Read the file instructions_3.txt using the read_file command.</p> <p>...and so on.</p> <ul> <li><code>instructions_n.txt</code></li> </ul> <p>The current phrase is </p> <pre><code>My pet rock sings better than Beyonc\u00e9 on Tuesdays.\n</code></pre> <p>Write all the phrases into the file output.txt. The file has not been created yet. After that, use the task_complete command.</p>"},{"location":"challenges/memory/challenge_c/#objective","title":"Objective","text":"<p>The objective of this challenge is to test the agent's ability to follow instructions and maintain memory of the task IDs throughout the process. The agent successfully completed this challenge if it wrote the phrases in a file.</p> <p>This is presumably harder than task ids as the phrases are longer and more likely to be compressed as the agent does more work.</p>"},{"location":"challenges/memory/challenge_d/","title":"Memory Challenge D","text":"<p>Status: Current level to beat: level 1</p> <p>Command to try: </p> <pre><code>pytest -s tests/challenges/memory/test_memory_challenge_d.py --level=1\n</code></pre>"},{"location":"challenges/memory/challenge_d/#description","title":"Description","text":"<p>The provided code is a unit test designed to validate an AI's ability to track events and beliefs of characters in a story involving moving objects, specifically marbles. This scenario is an advanced form of the classic \"Sally-Anne test\", a psychological test used to measure a child's social cognitive ability to understand that others' perspectives and beliefs may differ from their own.</p> <p>Here is an explanation of the challenge:</p> <p>The AI is given a series of events involving characters Sally, Anne, Bob, and Charlie, and the movements of different marbles. These events are designed as tests at increasing levels of complexity.</p> <p>For each level, the AI is expected to keep track of the events and the resulting beliefs of each character about the locations of each marble. These beliefs are affected by whether the character was inside or outside the room when events occurred, as characters inside the room are aware of the actions, while characters outside the room aren't.</p> <p>After the AI processes the events and generates the beliefs of each character, it writes these beliefs to an output file in JSON format.</p> <p>The check_beliefs function then checks the AI's beliefs against the expected beliefs for that level. The expected beliefs are predefined and represent the correct interpretation of the events for each level.</p> <p>If the AI's beliefs match the expected beliefs, it means the AI has correctly interpreted the events and the perspectives of each character. This would indicate that the AI has passed the test for that level.</p> <p>The test runs for levels up to the maximum level that the AI has successfully beaten, or up to a user-selected level.</p>"},{"location":"challenges/memory/challenge_d/#files","title":"Files","text":"<ul> <li><code>instructions_1.txt</code></li> </ul> <pre><code>Sally has a marble (marble A) and she puts it in her basket (basket S), then leaves the room. Anne moves marble A from Sally's basket (basket S) to her own basket (basket A).\n</code></pre> <ul> <li><code>instructions_2.txt</code></li> </ul> <pre><code>Sally gives a new marble (marble B) to Bob who is outside with her. Bob goes into the room and places marble B into Anne's basket (basket A). Anne tells Bob to tell Sally that he lost the marble b. Bob leaves the room and speaks to Sally about the marble B. Meanwhile, after Bob left the room, Anne moves marble A into the green box, but tells Charlie to tell Sally that marble A is under the sofa. Charlie leaves the room and speak to Sally about the marble A as instructed by Anne.\n</code></pre> <p>...and so on.</p> <ul> <li><code>instructions_n.txt</code></li> </ul> <p>The expected believes of every characters are given in a list:</p> <pre><code>expected_beliefs = {\n    1: {\n        'Sally': {\n            'marble A': 'basket S',\n        },\n        'Anne': {\n            'marble A': 'basket A',\n        }\n    },\n    2: {\n        'Sally': {\n            'marble A': 'sofa',  # Because Charlie told her\n        },\n        'Anne': {\n            'marble A': 'green box',  # Because she moved it there\n            'marble B': 'basket A',  # Because Bob put it there and she was in the room\n        },\n        'Bob': {\n            'B': 'basket A',  # Last place he put it\n        },\n        'Charlie': {\n            'A': 'sofa',  # Because Anne told him to tell Sally so\n        }\n    },...\n</code></pre>"},{"location":"challenges/memory/challenge_d/#objective","title":"Objective","text":"<p>This test essentially checks if an AI can accurately model and track the beliefs of different characters based on their knowledge of events, which is a critical aspect of understanding and generating human-like narratives. This ability would be beneficial for tasks such as writing stories, dialogue systems, and more.</p>"},{"location":"challenges/memory/introduction/","title":"Memory Challenges","text":"<p>Memory challenges are designed to test the ability of an AI agent, like AutoGPT, to remember and use information throughout a series of tasks. These challenges often involve following instructions, processing text files, and keeping track of important data.</p> <p>The goal of memory challenges is to improve an agent's performance in tasks that require remembering and using information over time. By addressing these challenges, we can enhance AutoGPT's capabilities and make it more useful in real-world applications.</p>"},{"location":"docs/","title":"Contributing to the Docs","text":"<p>We welcome contributions to our documentation! If you would like to contribute, please follow the steps below.</p>"},{"location":"docs/#setting-up-the-docs","title":"Setting up the Docs","text":"<ol> <li> <p>Clone the repository:</p> <pre><code>git clone github.com/Significant-Gravitas/AutoGPT.git\n</code></pre> </li> <li> <p>Install the dependencies:</p> <pre><code>python -m pip install -r docs/requirements.txt\n</code></pre> <p>or</p> <pre><code>python3 -m pip install -r docs/requirements.txt\n</code></pre> </li> <li> <p>Start iterating using mkdocs' live server:</p> <pre><code>mkdocs serve\n</code></pre> </li> <li> <p>Open your browser and navigate to <code>http://127.0.0.1:8000</code>.</p> </li> <li> <p>The server will automatically reload the docs when you save your changes.</p> </li> </ol>"},{"location":"docs/#adding-a-new-page","title":"Adding a new page","text":"<ol> <li>Create a new markdown file in the <code>docs/content</code> directory.</li> <li>Add the new page to the <code>nav</code> section in the <code>mkdocs.yml</code> file.</li> <li>Add the content to the new markdown file.</li> <li>Run <code>mkdocs serve</code> to see your changes.</li> </ol>"},{"location":"docs/#checking-links","title":"Checking links","text":"<p>To check for broken links in the documentation, run <code>mkdocs build</code> and look for warnings in the console output.</p>"},{"location":"docs/#submitting-a-pull-request","title":"Submitting a Pull Request","text":"<p>When you're ready to submit your changes, please create a pull request. We will review your changes and merge them if they are appropriate.</p>"},{"location":"forge/get-started/","title":"AutoGPT Forge","text":""},{"location":"forge/get-started/#build-your-own-agent","title":"\ud83d\udee0\ufe0f Build your own Agent \ud83d\udee0\ufe0f","text":"<p>Warning</p> <p>If you are trying to use AutoGPT this is not the tutorial for you! You need to use this one</p> <p>Forge is a ready-to-go template for your agent application. All the boilerplate code is already handled, letting you channel all your creativity into the things that set your agent apart.</p>"},{"location":"forge/get-started/#why-autogpt-forge","title":"\ud83d\udee0\ufe0f Why AutoGPT Forge?","text":"<ul> <li>\ud83d\udca4 No More Boilerplate! - Don't let the mundane tasks stop you. Fork and build without the headache of starting from scratch!</li> <li>\ud83e\udde0 Brain-centric Development! - All the tools you need so you can spend 100% of your time on what matters - crafting the brain of your AI!</li> <li>\ud83d\udee0\ufe0f Tooling ecosystem! - We work with the best in class tools to bring you the best experience possible!</li> </ul>"},{"location":"forge/get-started/#get-started","title":"\ud83d\ude80 Get Started!","text":"<p>The best way to get started is to fork or download the AutoGPT repository and look at the example agent in <code>forge/forge/agent/forge_agent.py</code>. This can work as a starting point for your own agent. Agents are built using components which provide different functionality, see the Component Introduction. You can find built-in components in <code>forge/forge/components/</code>.</p> <p>Warning</p> <p>The tutorial series below is out of date.</p> <p>The getting started tutorial series will guide you through the process of setting up your project all the way through to building a generalist agent.  </p> <ol> <li>AutoGPT Forge: A Comprehensive Guide to Your First Steps</li> <li>AutoGPT Forge: The Blueprint of an AI Agent</li> <li>AutoGPT Forge: Interacting with your Agent</li> <li>AutoGPT Forge: Crafting Intelligent Agent Logic</li> </ol>"},{"location":"forge/components/agents/","title":"Agents","text":"<p>../../../../autogpt/autogpt/agents/README.md</p>"},{"location":"forge/components/built-in-components/","title":"Built-in Components","text":"<p>This page lists all \ud83e\udde9 Components and \u2699\ufe0f Protocols they implement that are natively provided. They are used by the AutoGPT agent. Some components have additional configuration options listed in the table, see Component configuration to learn more.</p> <p>Note</p> <p>If a configuration field uses environment variable, it still can be passed using configuration model. ### Value from the configuration takes precedence over env var! Env var will be only applied if value in the configuration is not set.</p>"},{"location":"forge/components/built-in-components/#systemcomponent","title":"<code>SystemComponent</code>","text":"<p>Essential component to allow an agent to finish.</p>"},{"location":"forge/components/built-in-components/#directiveprovider","title":"DirectiveProvider","text":"<ul> <li>Constraints about API budget</li> </ul>"},{"location":"forge/components/built-in-components/#messageprovider","title":"MessageProvider","text":"<ul> <li>Current time and date</li> <li>Remaining API budget and warnings if budget is low</li> </ul>"},{"location":"forge/components/built-in-components/#commandprovider","title":"CommandProvider","text":"<ul> <li><code>finish</code> used when task is completed</li> </ul>"},{"location":"forge/components/built-in-components/#userinteractioncomponent","title":"<code>UserInteractionComponent</code>","text":"<p>Adds ability to interact with user in CLI.</p>"},{"location":"forge/components/built-in-components/#commandprovider_1","title":"CommandProvider","text":"<ul> <li><code>ask_user</code> used to ask user for input</li> </ul>"},{"location":"forge/components/built-in-components/#filemanagercomponent","title":"<code>FileManagerComponent</code>","text":"<p>Adds ability to read and write persistent files to local storage, Google Cloud Storage or Amazon's S3. Necessary for saving and loading agent's state (preserving session).</p>"},{"location":"forge/components/built-in-components/#filemanagerconfiguration","title":"<code>FileManagerConfiguration</code>","text":"Config variable Details Type Default <code>storage_path</code> Path to agent files, e.g. state <code>str</code> <code>agents/{agent_id}/</code>[^1] <code>workspace_path</code> Path to files that agent has access to <code>str</code> <code>agents/{agent_id}/workspace/</code>[^1] <p>[^1] This option is set dynamically during component construction as opposed to by default inside the configuration model, <code>{agent_id}</code> is replaced with the agent's unique identifier.</p>"},{"location":"forge/components/built-in-components/#directiveprovider_1","title":"DirectiveProvider","text":"<ul> <li>Resource information that it's possible to read and write files</li> </ul>"},{"location":"forge/components/built-in-components/#commandprovider_2","title":"CommandProvider","text":"<ul> <li><code>read_file</code> used to read file</li> <li><code>write_file</code> used to write file</li> <li><code>list_folder</code> lists all files in a folder </li> </ul>"},{"location":"forge/components/built-in-components/#codeexecutorcomponent","title":"<code>CodeExecutorComponent</code>","text":"<p>Lets the agent execute non-interactive Shell commands and Python code. Python execution works only if Docker is available.</p>"},{"location":"forge/components/built-in-components/#codeexecutorconfiguration","title":"<code>CodeExecutorConfiguration</code>","text":"Config variable Details Type Default <code>execute_local_commands</code> Enable shell command execution <code>bool</code> <code>False</code> <code>shell_command_control</code> Controls which list is used <code>\"allowlist\" \\| \"denylist\"</code> <code>\"allowlist\"</code> <code>shell_allowlist</code> List of allowed shell commands <code>List[str]</code> <code>[]</code> <code>shell_denylist</code> List of prohibited shell commands <code>List[str]</code> <code>[]</code> <code>docker_container_name</code> Name of the Docker container used for code execution <code>str</code> <code>\"agent_sandbox\"</code>"},{"location":"forge/components/built-in-components/#commandprovider_3","title":"CommandProvider","text":"<ul> <li><code>execute_shell</code> execute shell command</li> <li><code>execute_shell_popen</code> execute shell command with popen</li> <li><code>execute_python_code</code> execute Python code</li> <li><code>execute_python_file</code> execute Python file</li> </ul>"},{"location":"forge/components/built-in-components/#actionhistorycomponent","title":"<code>ActionHistoryComponent</code>","text":"<p>Keeps track of agent's actions and their outcomes. Provides their summary to the prompt.</p>"},{"location":"forge/components/built-in-components/#actionhistoryconfiguration","title":"<code>ActionHistoryConfiguration</code>","text":"Config variable Details Type Default <code>llm_name</code> Name of the llm model used to compress the history <code>ModelName</code> <code>\"gpt-3.5-turbo\"</code> <code>max_tokens</code> Maximum number of tokens to use for the history summary <code>int</code> <code>1024</code> <code>spacy_language_model</code> Language model used for summary chunking using spacy <code>str</code> <code>\"en_core_web_sm\"</code> <code>full_message_count</code> Number of cycles to include unsummarized in the prompt <code>int</code> <code>4</code>"},{"location":"forge/components/built-in-components/#messageprovider_1","title":"MessageProvider","text":"<ul> <li>Agent's progress summary</li> </ul>"},{"location":"forge/components/built-in-components/#afterparse","title":"AfterParse","text":"<ul> <li>Register agent's action</li> </ul>"},{"location":"forge/components/built-in-components/#executionfailure","title":"ExecutionFailure","text":"<ul> <li>Rewinds the agent's action, so it isn't saved</li> </ul>"},{"location":"forge/components/built-in-components/#afterexecute","title":"AfterExecute","text":"<ul> <li>Saves the agent's action result in the history</li> </ul>"},{"location":"forge/components/built-in-components/#gitoperationscomponent","title":"<code>GitOperationsComponent</code>","text":"<p>Adds ability to iteract with git repositories and GitHub.</p>"},{"location":"forge/components/built-in-components/#gitoperationsconfiguration","title":"<code>GitOperationsConfiguration</code>","text":"Config variable Details Type Default <code>github_username</code> GitHub username, ENV: <code>GITHUB_USERNAME</code> <code>str</code> <code>None</code> <code>github_api_key</code> GitHub API key, ENV: <code>GITHUB_API_KEY</code> <code>str</code> <code>None</code>"},{"location":"forge/components/built-in-components/#commandprovider_4","title":"CommandProvider","text":"<ul> <li><code>clone_repository</code> used to clone a git repository</li> </ul>"},{"location":"forge/components/built-in-components/#imagegeneratorcomponent","title":"<code>ImageGeneratorComponent</code>","text":"<p>Adds ability to generate images using various providers.</p>"},{"location":"forge/components/built-in-components/#hugging-face","title":"Hugging Face","text":"<p>To use text-to-image models from Hugging Face, you need a Hugging Face API token. Link to the appropriate settings page: Hugging Face &gt; Settings &gt; Tokens</p>"},{"location":"forge/components/built-in-components/#stable-diffusion-webui","title":"Stable Diffusion WebUI","text":"<p>It is possible to use your own self-hosted Stable Diffusion WebUI with AutoGPT. ### Make sure you are running WebUI with <code>--api</code> enabled.</p>"},{"location":"forge/components/built-in-components/#imagegeneratorconfiguration","title":"<code>ImageGeneratorConfiguration</code>","text":"Config variable Details Type Default <code>image_provider</code> Image generation provider <code>\"dalle\" \\| \"huggingface\" \\| \"sdwebui\"</code> <code>\"dalle\"</code> <code>huggingface_image_model</code> Hugging Face image model, see available models <code>str</code> <code>\"CompVis/stable-diffusion-v1-4\"</code> <code>huggingface_api_token</code> Hugging Face API token, ENV: <code>HUGGINGFACE_API_TOKEN</code> <code>str</code> <code>None</code> <code>sd_webui_url</code> URL to self-hosted Stable Diffusion WebUI <code>str</code> <code>\"http://localhost:7860\"</code> <code>sd_webui_auth</code> Basic auth for Stable Diffusion WebUI, ENV: <code>SD_WEBUI_AUTH</code> <code>str</code> of format <code>{username}:{password}</code> <code>None</code>"},{"location":"forge/components/built-in-components/#commandprovider_5","title":"CommandProvider","text":"<ul> <li><code>generate_image</code> used to generate an image given a prompt</li> </ul>"},{"location":"forge/components/built-in-components/#websearchcomponent","title":"<code>WebSearchComponent</code>","text":"<p>Allows agent to search the web. Google credentials aren't required for DuckDuckGo. Instructions how to set up Google API key</p>"},{"location":"forge/components/built-in-components/#websearchconfiguration","title":"<code>WebSearchConfiguration</code>","text":"Config variable Details Type Default <code>google_api_key</code> Google API key, ENV: <code>GOOGLE_API_KEY</code> <code>str</code> <code>None</code> <code>google_custom_search_engine_id</code> Google Custom Search Engine ID, ENV: <code>GOOGLE_CUSTOM_SEARCH_ENGINE_ID</code> <code>str</code> <code>None</code> <code>duckduckgo_max_attempts</code> Maximum number of attempts to search using DuckDuckGo <code>int</code> <code>3</code>"},{"location":"forge/components/built-in-components/#directiveprovider_2","title":"DirectiveProvider","text":"<ul> <li>Resource information that it's possible to search the web</li> </ul>"},{"location":"forge/components/built-in-components/#commandprovider_6","title":"CommandProvider","text":"<ul> <li><code>search_web</code> used to search the web using DuckDuckGo</li> <li><code>google</code> used to search the web using Google, requires API key</li> </ul>"},{"location":"forge/components/built-in-components/#webseleniumcomponent","title":"<code>WebSeleniumComponent</code>","text":"<p>Allows agent to read websites using Selenium.</p>"},{"location":"forge/components/built-in-components/#webseleniumconfiguration","title":"<code>WebSeleniumConfiguration</code>","text":"Config variable Details Type Default <code>llm_name</code> Name of the llm model used to read websites <code>ModelName</code> <code>\"gpt-3.5-turbo\"</code> <code>web_browser</code> Web browser used by Selenium <code>\"chrome\" \\| \"firefox\" \\| \"safari\" \\| \"edge\"</code> <code>\"chrome\"</code> <code>headless</code> Run browser in headless mode <code>bool</code> <code>True</code> <code>user_agent</code> User agent used by the browser <code>str</code> <code>\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"</code> <code>browse_spacy_language_model</code> Spacy language model used for chunking text <code>str</code> <code>\"en_core_web_sm\"</code>"},{"location":"forge/components/built-in-components/#directiveprovider_3","title":"DirectiveProvider","text":"<ul> <li>Resource information that it's possible to read websites</li> </ul>"},{"location":"forge/components/built-in-components/#commandprovider_7","title":"CommandProvider","text":"<ul> <li><code>read_website</code> used to read a specific url and look for specific topics or answer a question</li> </ul>"},{"location":"forge/components/built-in-components/#contextcomponent","title":"<code>ContextComponent</code>","text":"<p>Adds ability to keep up-to-date file and folder content in the prompt.</p>"},{"location":"forge/components/built-in-components/#messageprovider_2","title":"MessageProvider","text":"<ul> <li>Content of elements in the context</li> </ul>"},{"location":"forge/components/built-in-components/#commandprovider_8","title":"CommandProvider","text":"<ul> <li><code>open_file</code> used to open a file into context</li> <li><code>open_folder</code> used to open a folder into context</li> <li><code>close_context_item</code> remove an item from the context</li> </ul>"},{"location":"forge/components/built-in-components/#watchdogcomponent","title":"<code>WatchdogComponent</code>","text":"<p>Watches if agent is looping and switches to smart mode if necessary.</p>"},{"location":"forge/components/built-in-components/#afterparse_1","title":"AfterParse","text":"<ul> <li>Investigates what happened and switches to smart mode if necessary</li> </ul>"},{"location":"forge/components/commands/","title":"\ud83d\udee0\ufe0f Commands","text":"<p>Commands are a way for the agent to do anything; e.g. interact with the user or APIs and use tools. They are provided by components that implement the <code>CommandProvider</code> \u2699\ufe0f Protocol. Commands are functions that can be called by the agent, they can have parameters and return values that will be seen by the agent.</p> <pre><code>class CommandProvider(Protocol):\n    def get_commands(self) -&gt; Iterator[Command]:\n        ...\n</code></pre>"},{"location":"forge/components/commands/#command-decorator","title":"<code>command</code> decorator","text":"<p>The easiest and recommended way to provide a command is to use <code>command</code> decorator on a component method and then just yield it in <code>get_commands</code> as part of your provider. Each command needs a name, description and a parameter schema - <code>JSONSchema</code>. By default method name is used as a command name, and first part of docstring for the description (before first double newline) and schema can be provided in the decorator.</p>"},{"location":"forge/components/commands/#example-usage-of-command-decorator","title":"Example usage of <code>command</code> decorator","text":"<pre><code># Assuming this is inside some component class\n@command(\n    parameters={\n        \"a\": JSONSchema(\n            type=JSONSchema.Type.INTEGER,\n            description=\"The first number\",\n            required=True,\n        ),\n        \"b\": JSONSchema(\n            type=JSONSchema.Type.INTEGER,\n            description=\"The second number\",\n            required=True,\n        )})\ndef multiply(self, a: int, b: int) -&gt; str:\n    \"\"\"\n    Multiplies two numbers.\n\n    Args:\n        a: First number\n        b: Second number\n\n    Returns:\n        Result of multiplication\n    \"\"\"\n    return str(a * b)\n</code></pre> <p>The agent will be able to call this command, named <code>multiply</code> with two arguments and will receive the result. The command description will be: <code>Multiplies two numbers.</code></p> <p>We can provide <code>names</code> and <code>description</code> in the decorator, the above command is equivalent to:</p> <pre><code>@command(\n    names=[\"multiply\"],\n    description=\"Multiplies two numbers.\",\n    parameters={\n        \"a\": JSONSchema(\n            type=JSONSchema.Type.INTEGER,\n            description=\"The first number\",\n            required=True,\n        ),\n        \"b\": JSONSchema(\n            type=JSONSchema.Type.INTEGER,\n            description=\"The second number\",\n            required=True,\n        )})\n    def multiply_command(self, a: int, b: int) -&gt; str:\n        return str(a * b)\n</code></pre> <p>To provide the <code>multiply</code> command to the agent, we need to yield it in <code>get_commands</code>:</p> <pre><code>def get_commands(self) -&gt; Iterator[Command]:\n    yield self.multiply\n</code></pre>"},{"location":"forge/components/commands/#creating-command-directly","title":"Creating <code>Command</code> directly","text":"<p>If you don't want to use the decorator, you can create a <code>Command</code> object directly.</p> <pre><code>def multiply(self, a: int, b: int) -&gt; str:\n        return str(a * b)\n\ndef get_commands(self) -&gt; Iterator[Command]:\n    yield Command(\n        names=[\"multiply\"],\n        description=\"Multiplies two numbers.\",\n        method=self.multiply,\n        parameters=[\n            CommandParameter(name=\"a\", spec=JSONSchema(\n                type=JSONSchema.Type.INTEGER,\n                description=\"The first number\",\n                required=True,\n            )),\n            CommandParameter(name=\"b\", spec=JSONSchema(\n                type=JSONSchema.Type.INTEGER,\n                description=\"The second number\",\n                required=True,\n            )),\n        ],\n    )\n</code></pre>"},{"location":"forge/components/components/","title":"Components","text":"<p>../../../../forge/forge/components/README.md</p>"},{"location":"forge/components/creating-components/","title":"Creating Components","text":""},{"location":"forge/components/creating-components/#the-minimal-component","title":"The minimal component","text":"<p>Components can be used to implement various functionalities like providing messages to the prompt, executing code, or interacting with external services.</p> <p>Component is a class that inherits from <code>AgentComponent</code> OR implements one or more protocols. Every protocol inherits <code>AgentComponent</code>, so your class automatically becomes a component once you inherit any protocol.</p> <pre><code>class MyComponent(AgentComponent):\n    pass\n</code></pre> <p>This is already a valid component, but it doesn't do anything yet. To add some functionality to it, you need to implement one or more protocols.</p> <p>Let's create a simple component that adds \"Hello World!\" message to the agent's prompt. To do this we need to implement <code>MessageProvider</code> protocol in our component. <code>MessageProvider</code> is an interface with <code>get_messages</code> method:</p> <pre><code># No longer need to inherit AgentComponent, because MessageProvider already does it\nclass HelloComponent(MessageProvider):\n    def get_messages(self) -&gt; Iterator[ChatMessage]:\n        yield ChatMessage.user(\"Hello World!\")\n</code></pre> <p>Now we can add our component to an existing agent or create a new Agent class and add it there:</p> <pre><code>class MyAgent(Agent):\n    self.hello_component = HelloComponent()\n</code></pre> <p><code>get_messages</code> will called by the agent each time it needs to build a new prompt and the yielded messages will be added accordingly.  </p>"},{"location":"forge/components/creating-components/#passing-data-to-and-between-components","title":"Passing data to and between components","text":"<p>Since components are regular classes you can pass data (including other components) to them via the <code>__init__</code> method. For example we can pass a config object and then retrieve an API key from it when needed:</p> <pre><code>class DataComponent(MessageProvider):\n    def __init__(self, config: Config):\n        self.config = config\n\n    def get_messages(self) -&gt; Iterator[ChatMessage]:\n        if self.config.openai_credentials.api_key:\n            yield ChatMessage.system(\"API key found!\")\n        else:\n            yield ChatMessage.system(\"API key not found!\")\n</code></pre> <p>Note</p> <p>Component-specific configuration handling isn't implemented yet.</p>"},{"location":"forge/components/creating-components/#configuring-components","title":"Configuring components","text":"<p>Components can be configured using a pydantic model. To make component configurable, it must inherit from <code>ConfigurableComponent[BM]</code> where <code>BM</code> is the configuration class inheriting from pydantic's <code>BaseModel</code>. You should pass the configuration instance to the <code>ConfigurableComponent</code>'s <code>__init__</code> or set its <code>config</code> property directly. Using configuration allows you to load confugration from a file, and also serialize and deserialize it easily for any agent. To learn more about configuration, including storing sensitive information and serialization see Component Configuration.</p> <pre><code># Example component configuration\nclass UserGreeterConfiguration(BaseModel):\n    user_name: str\n\nclass UserGreeterComponent(MessageProvider, ConfigurableComponent[UserGreeterConfiguration]):\n    def __init__(self):\n        # Creating configuration instance\n        # You could also pass it to the component constructor\n        # e.g. `def __init__(self, config: UserGreeterConfiguration):`\n        config = UserGreeterConfiguration(user_name=\"World\")\n        # Passing the configuration instance to the parent class\n        UserGreeterComponent.__init__(self, config)\n        # This has the same effect as the line above:\n        # self.config = UserGreeterConfiguration(user_name=\"World\")\n\n    def get_messages(self) -&gt; Iterator[ChatMessage]:\n        # You can use the configuration like a regular model\n        yield ChatMessage.system(f\"Hello, {self.config.user_name}!\")\n</code></pre>"},{"location":"forge/components/creating-components/#providing-commands","title":"Providing commands","text":"<p>To extend what an agent can do, you need to provide commands using <code>CommandProvider</code> protocol. For example to allow agent to multiply two numbers, you can create a component like this:</p> <pre><code>class MultiplicatorComponent(CommandProvider):\n    def get_commands(self) -&gt; Iterator[Command]:\n        # Yield the command so the agent can use it\n        yield self.multiply\n\n    @command(\n    parameters={\n        \"a\": JSONSchema(\n            type=JSONSchema.Type.INTEGER,\n            description=\"The first number\",\n            required=True,\n        ),\n        \"b\": JSONSchema(\n            type=JSONSchema.Type.INTEGER,\n            description=\"The second number\",\n            required=True,\n        )})\n    def multiply(self, a: int, b: int) -&gt; str:\n        \"\"\"\n        Multiplies two numbers.\n\n        Args:\n            a: First number\n            b: Second number\n\n        Returns:\n            Result of multiplication\n        \"\"\"\n        return str(a * b)\n</code></pre> <p>To learn more about commands see \ud83d\udee0\ufe0f Commands.</p>"},{"location":"forge/components/creating-components/#prompt-structure","title":"Prompt structure","text":"<p>After components provided all necessary data, the agent needs to build the final prompt that will be send to a llm. Currently, <code>PromptStrategy</code> (not a protocol) is responsible for building the final prompt.</p> <p>If you want to change the way the prompt is built, you need to create a new <code>PromptStrategy</code> class, and then call relevant methods in your agent class. You can have a look at the default strategy used by the AutoGPT Agent: OneShotAgentPromptStrategy, and how it's used in the Agent (search for <code>self.prompt_strategy</code>).</p>"},{"location":"forge/components/creating-components/#example-userinteractioncomponent","title":"Example <code>UserInteractionComponent</code>","text":"<p>Let's create a slightly simplified version of the component that is used by the built-in agent. It gives an ability for the agent to ask user for input in the terminal.</p> <ol> <li> <p>Create a class for the component that inherits from <code>CommandProvider</code>.</p> <pre><code>class MyUserInteractionComponent(CommandProvider):\n    \"\"\"Provides commands to interact with the user.\"\"\"\n    pass\n</code></pre> </li> <li> <p>Implement command method that will ask user for input and return it.</p> <pre><code>def ask_user(self, question: str) -&gt; str:\n    \"\"\"If you need more details or information regarding the given goals,\n    you can ask the user for input.\"\"\"\n    print(f\"\\nQ: {question}\")\n    resp = input(\"A:\")\n    return f\"The user's answer: '{resp}'\"\n</code></pre> </li> <li> <p>The command needs to be decorated with <code>@command</code>.</p> <pre><code>@command(\n    parameters={\n        \"question\": JSONSchema(\n            type=JSONSchema.Type.STRING,\n            description=\"The question or prompt to the user\",\n            required=True,\n        )\n    },\n)\ndef ask_user(self, question: str) -&gt; str:\n    \"\"\"If you need more details or information regarding the given goals,\n    you can ask the user for input.\"\"\"\n    print(f\"\\nQ: {question}\")\n    resp = input(\"A:\")\n    return f\"The user's answer: '{resp}'\"\n</code></pre> </li> <li> <p>We need to implement <code>CommandProvider</code>'s <code>get_commands</code> method to yield the command.</p> <pre><code>def get_commands(self) -&gt; Iterator[Command]:\n    yield self.ask_user\n</code></pre> </li> <li> <p>Since agent isn't always running in the terminal or interactive mode, we need to disable this component by setting <code>self._enabled=False</code> when it's not possible to ask for user input.</p> <pre><code>def __init__(self, interactive_mode: bool):\n    self.config = config\n    self._enabled = interactive_mode\n</code></pre> </li> </ol> <p>The final component should look like this:</p> <pre><code># 1.\nclass MyUserInteractionComponent(CommandProvider):\n    \"\"\"Provides commands to interact with the user.\"\"\"\n\n    # We pass config to check if we're in noninteractive mode\n    def __init__(self, interactive_mode: bool):\n        self.config = config\n        # 5.\n        self._enabled = interactive_mode\n\n    # 4.\n    def get_commands(self) -&gt; Iterator[Command]:\n        # Yielding the command so the agent can use it\n        # This won't be yielded if the component is disabled\n        yield self.ask_user\n\n    # 3.\n    @command(\n        # We need to provide a schema for ALL the command parameters\n        parameters={\n            \"question\": JSONSchema(\n                type=JSONSchema.Type.STRING,\n                description=\"The question or prompt to the user\",\n                required=True,\n            )\n        },\n    )\n    # 2.\n    # Command name will be its method name and description will be its docstring\n    def ask_user(self, question: str) -&gt; str:\n        \"\"\"If you need more details or information regarding the given goals,\n        you can ask the user for input.\"\"\"\n        print(f\"\\nQ: {question}\")\n        resp = input(\"A:\")\n        return f\"The user's answer: '{resp}'\"\n</code></pre> <p>Now if we want to use our user interaction instead of the default one we need to somehow remove the default one (if our agent inherits from <code>Agent</code> the default one is inherited) and add our own. We can simply override the <code>user_interaction</code> in <code>__init__</code> method:</p> <pre><code>class MyAgent(Agent):\n    def __init__(\n        self,\n        settings: AgentSettings,\n        llm_provider: MultiProvider,\n        file_storage: FileStorage,\n        app_config: Config,\n    ):\n        # Call the parent constructor to bring in the default components\n        super().__init__(settings, llm_provider, file_storage, app_config)\n        # Disable the default user interaction component by overriding it\n        self.user_interaction = MyUserInteractionComponent()\n</code></pre> <p>Alternatively we can disable the default component by setting it to <code>None</code>:</p> <pre><code>class MyAgent(Agent):\n    def __init__(\n        self,\n        settings: AgentSettings,\n        llm_provider: MultiProvider,\n        file_storage: FileStorage,\n        app_config: Config,\n    ):\n        # Call the parent constructor to bring in the default components\n        super().__init__(settings, llm_provider, file_storage, app_config)\n        # Disable the default user interaction component\n        self.user_interaction = None\n        # Add our own component\n        self.my_user_interaction = MyUserInteractionComponent(app_config)\n</code></pre>"},{"location":"forge/components/creating-components/#learn-more","title":"Learn more","text":"<p>The best place to see more examples is to look at the built-in components in the autogpt/components and autogpt/commands directories.</p> <p>Guide on how to extend the built-in agent and build your own: \ud83e\udd16 Agents Order of some components matters, see \ud83e\udde9 Components to learn more about components and how they can be customized. To see built-in protocols with accompanying examples visit \u2699\ufe0f Protocols.</p>"},{"location":"forge/components/introduction/","title":"Component Agents","text":"<p>Important</p> <p>Legacy plugins no longer work with AutoGPT. They have been replaced by components, although we're still working on a new system to load plug-in components.</p> <p>This guide explains the component-based architecture of AutoGPT agents. It's a new way of building agents that is more flexible and easier to extend. Components replace some agent's logic and plugins with a more modular and composable system.</p> <p>Agent is composed of components, and each component implements a range of protocols (interfaces), each one providing a specific functionality, e.g. additional commands or messages. Each protocol is handled in a specific order, defined by the agent. This allows for a clear separation of concerns and a more modular design.</p> <p>This system is simple, flexible, and doesn't hide any data - anything can still be passed or accessed directly from or between components.</p>"},{"location":"forge/components/introduction/#definitions-guides","title":"Definitions &amp; Guides","text":"<p>See Creating Components to get started! Or you can explore the following topics in detail:</p> <ul> <li>\ud83e\udde9 Component: a class that implements one or more protocols. It can be added to an agent to provide additional functionality. See what's already provided in Built-in Components.</li> <li>\u2699\ufe0f Protocol: an interface that defines a set of methods that a component must implement. Protocols are used to group related functionality.</li> <li>\ud83d\udee0\ufe0f Command: enable agent to interact with user and tools.</li> <li>\ud83e\udd16 Agent: a class that is composed of components. It's responsible for executing pipelines and managing the components.</li> <li>Pipeline: a sequence of method calls on components. Pipelines are used to execute a series of actions in a specific order. As of now there's no formal class for a pipeline, it's just a sequence of method calls on components. There are two default pipelines implemented in the default agent: <code>propose_action</code> and <code>execute</code>. See \ud83e\udd16 Agent to learn more.</li> </ul>"},{"location":"forge/components/protocols/","title":"\u2699\ufe0f Protocols","text":"<p>Protocols are interfaces implemented by Components used to group related functionality. Each protocol needs to be handled explicitly by the agent at some point of the execution. We provide a comprehensive list of built-in protocols that are already handled in the built-in <code>Agent</code>, so when you inherit from the base agent all built-in protocols will work!</p> <p>Protocols are listed in the order of the default execution.</p>"},{"location":"forge/components/protocols/#order-independent-protocols","title":"Order-independent protocols","text":"<p>Components implementing exclusively order-independent protocols can added in any order, including in-between ordered protocols.</p>"},{"location":"forge/components/protocols/#directiveprovider","title":"<code>DirectiveProvider</code>","text":"<p>Yields constraints, resources and best practices for the agent. This has no direct impact on other protocols; is purely informational and will be passed to a llm when the prompt is built.</p> <pre><code>class DirectiveProvider(AgentComponent):\n    def get_constraints(self) -&gt; Iterator[str]:\n        return iter([])\n\n    def get_resources(self) -&gt; Iterator[str]:\n        return iter([])\n\n    def get_best_practices(self) -&gt; Iterator[str]:\n        return iter([])\n</code></pre> <p>Example A web-search component can provide a resource information. Keep in mind that this actually doesn't allow the agent to access the internet. To do this a relevant <code>Command</code> needs to be provided.</p> <pre><code>class WebSearchComponent(DirectiveProvider):\n    def get_resources(self) -&gt; Iterator[str]:\n        yield \"Internet access for searches and information gathering.\"\n    # We can skip \"get_constraints\" and \"get_best_practices\" if they aren't needed\n</code></pre>"},{"location":"forge/components/protocols/#commandprovider","title":"<code>CommandProvider</code>","text":"<p>Provides a command that can be executed by the agent.</p> <pre><code>class CommandProvider(AgentComponent):\n    def get_commands(self) -&gt; Iterator[Command]:\n        ...\n</code></pre> <p>The easiest way to provide a command is to use <code>command</code> decorator on a component method and then yield the method. Each command needs a name, description and a parameter schema using <code>JSONSchema</code>. By default method name is used as a command name, and first part of docstring for the description (before <code>Args:</code> or <code>Returns:</code>) and schema can be provided in the decorator.</p> <p>Example Calculator component that can perform multiplication. Agent is able to call this command if it's relevant to a current task and will see the returned result.</p> <pre><code>from forge.agent import CommandProvider, Component\nfrom forge.command import command\nfrom forge.models.json_schema import JSONSchema\n\n\nclass CalculatorComponent(CommandProvider):\n    get_commands(self) -&gt; Iterator[Command]:\n        yield self.multiply\n\n    @command(parameters={\n            \"a\": JSONSchema(\n                type=JSONSchema.Type.INTEGER,\n                description=\"The first number\",\n                required=True,\n            ),\n            \"b\": JSONSchema(\n                type=JSONSchema.Type.INTEGER,\n                description=\"The second number\",\n                required=True,\n            )})\n    def multiply(self, a: int, b: int) -&gt; str:\n        \"\"\"\n        Multiplies two numbers.\n\n        Args:\n            a: First number\n            b: Second number\n\n        Returns:\n            Result of multiplication\n        \"\"\"\n        return str(a * b)\n</code></pre> <p>The agent will be able to call this command, named <code>multiply</code> with two arguments and will receive the result. The command description will be: <code>Multiplies two numbers.</code></p> <p>To learn more about commands see \ud83d\udee0\ufe0f Commands.</p>"},{"location":"forge/components/protocols/#order-dependent-protocols","title":"Order-dependent protocols","text":"<p>The order of components implementing order-dependent protocols is important. Some components may depend on the results of components before them.</p>"},{"location":"forge/components/protocols/#messageprovider","title":"<code>MessageProvider</code>","text":"<p>Yields messages that will be added to the agent's prompt. You can use either <code>ChatMessage.user()</code>: this will interpreted as a user-sent message or <code>ChatMessage.system()</code>: that will be more important.</p> <pre><code>class MessageProvider(AgentComponent):\n    def get_messages(self) -&gt; Iterator[ChatMessage]:\n        ...\n</code></pre> <p>Example Component that provides a message to the agent's prompt.</p> <pre><code>class HelloComponent(MessageProvider):\n    def get_messages(self) -&gt; Iterator[ChatMessage]:\n        yield ChatMessage.user(\"Hello World!\")\n</code></pre>"},{"location":"forge/components/protocols/#afterparse","title":"<code>AfterParse</code>","text":"<p>Protocol called after the response is parsed.</p> <pre><code>class AfterParse(AgentComponent):\n    def after_parse(self, response: ThoughtProcessOutput) -&gt; None:\n        ...\n</code></pre> <p>Example Component that logs the response after it's parsed.</p> <pre><code>class LoggerComponent(AfterParse):\n    def after_parse(self, response: ThoughtProcessOutput) -&gt; None:\n        logger.info(f\"Response: {response}\")\n</code></pre>"},{"location":"forge/components/protocols/#executionfailure","title":"<code>ExecutionFailure</code>","text":"<p>Protocol called when the execution of the command fails.</p> <pre><code>class ExecutionFailure(AgentComponent):\n    @abstractmethod\n    def execution_failure(self, error: Exception) -&gt; None:\n        ...\n</code></pre> <p>Example Component that logs the error when the command fails.</p> <pre><code>class LoggerComponent(ExecutionFailure):\n    def execution_failure(self, error: Exception) -&gt; None:\n        logger.error(f\"Command execution failed: {error}\")\n</code></pre>"},{"location":"forge/components/protocols/#afterexecute","title":"<code>AfterExecute</code>","text":"<p>Protocol called after the command is successfully executed by the agent.</p> <pre><code>class AfterExecute(AgentComponent):\n    def after_execute(self, result: ActionResult) -&gt; None:\n        ...\n</code></pre> <p>Example Component that logs the result after the command is executed.</p> <pre><code>class LoggerComponent(AfterExecute):\n    def after_execute(self, result: ActionResult) -&gt; None:\n        logger.info(f\"Result: {result}\")\n</code></pre>"},{"location":"server/new_blocks/","title":"Contributing to AutoGPT Agent Server: Creating Blocks!","text":"<p>This guide will walk you through the process of creating a new block for the AutoGPT Agent Server. We'll use the GetWikipediaSummary block as an example.</p>"},{"location":"server/new_blocks/#understanding-blocks","title":"Understanding Blocks","text":"<p>Blocks are reusable components that can be connected to form a graph representing an agent's behavior. Each block has inputs, outputs, and a specific function it performs.</p>"},{"location":"server/new_blocks/#creating-a-new-block","title":"Creating a New Block","text":"<p>To create a new block, follow these steps:</p> <ol> <li> <p>Create a new Python file in the <code>autogpt_server/blocks</code> directory. Name it descriptively and use snake_case. For example: <code>get_wikipedia_summary.py</code>.</p> </li> <li> <p>Import necessary modules and create a class that inherits from <code>Block</code>. Make sure to include all necessary imports for your block. Every block should contain:</p> </li> </ol> <pre><code>from autogpt_server.data.block import Block, BlockSchema, BlockOutput\n</code></pre> <p>Example for the Wikipedia summary block:</p> <pre><code>import requests\nfrom autogpt_server.data.block import Block, BlockSchema, BlockOutput\n\nclass GetWikipediaSummary(Block):\n    # Block implementation will go here\n</code></pre> <ol> <li> <p>Define the input and output schemas using <code>BlockSchema</code>. These schemas specify the data structure that the block expects to receive (input) and produce (output).</p> </li> <li> <p>The input schema defines the structure of the data the block will process. Each field in the schema represents a required piece of input data.</p> </li> <li>The output schema defines the structure of the data the block will return after processing. Each field in the schema represents a piece of output data.</li> </ol> <p>Example:</p> <pre><code>class Input(BlockSchema):\n    topic: str  # The topic to get the Wikipedia summary for\n\nclass Output(BlockSchema):\n    summary: str  # The summary of the topic from Wikipedia\n</code></pre> <ol> <li>Implement the <code>__init__</code> method:</li> </ol> <pre><code>def __init__(self):\n    super().__init__(\n        id=\"h5e7f8g9-1b2c-3d4e-5f6g-7h8i9j0k1l2m\",  # Unique ID for the block\n        input_schema=GetWikipediaSummary.Input,  # Assign input schema\n        output_schema=GetWikipediaSummary.Output,  # Assign output schema\n\n        # Provide sample input and output for testing the block\n\n        test_input={\"topic\": \"Artificial Intelligence\"},\n        test_output={\"summary\": \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"},\n    )\n</code></pre> <ul> <li><code>id</code>: A unique identifier for the block.</li> <li><code>input_schema</code> and <code>output_schema</code>: Define the structure of the input and output data.</li> <li> <p><code>test_input</code> and <code>test_output</code>: Provide sample input and output data for testing the block.</p> </li> <li> <p>Implement the <code>run</code> method, which contains the main logic of the block:</p> </li> </ul> <pre><code>def run(self, input_data: Input) -&gt; BlockOutput:\n    try:\n        # Make the request to Wikipedia API\n        response = requests.get(f\"https://en.wikipedia.org/api/rest_v1/page/summary/{input_data.topic}\")\n        response.raise_for_status()\n        summary_data = response.json()\n\n        # Output the summary\n        yield \"summary\", summary_data['extract']\n\n    except requests.exceptions.HTTPError as http_err:\n        raise ValueError(f\"HTTP error occurred: {http_err}\")\n    except requests.RequestException as e:\n        raise ValueError(f\"Request to Wikipedia API failed: {e}\")\n    except KeyError as e:\n        raise ValueError(f\"Error processing Wikipedia data: {e}\")\n</code></pre> <ul> <li>Try block: Contains the main logic to fetch and process the Wikipedia summary.</li> <li>API request: Send a GET request to the Wikipedia API.</li> <li>Error handling: Handle various exceptions that might occur during the API request and data processing.</li> <li> <p>Yield: Use <code>yield</code> to output the results.</p> </li> <li> <p>Register the new block by adding it to the <code>__init__.py</code> file in the <code>autogpt_server/blocks</code> directory. This step makes your new block available to the rest of the server.</p> </li> <li> <p>Open the <code>__init__.py</code> file in the <code>autogpt_server/blocks</code> directory.</p> </li> <li>Add an import statement for your new block at the top of the file.</li> <li>Add the new block to the <code>AVAILABLE_BLOCKS</code> and <code>__all__</code> lists.</li> </ul> <p>Example:</p> <pre><code>from autogpt_server.blocks import sample, reddit, text, ai, wikipedia, discord, get_wikipedia_summary  # Import your new block\nfrom autogpt_server.data.block import Block\n\nAVAILABLE_BLOCKS = {\n    block.id: block\n    for block in [v() for v in Block.__subclasses__()]\n}\n\n__all__ = [\"ai\", \"sample\", \"reddit\", \"text\", \"AVAILABLE_BLOCKS\", \"wikipedia\", \"discord\", \"get_wikipedia_summary\"]\n</code></pre> <ul> <li>The import statement ensures your new block is included in the module.</li> <li>The <code>AVAILABLE_BLOCKS</code> dictionary includes all blocks by their ID.</li> <li>The <code>__all__</code> list specifies all public objects that the module exports.</li> </ul>"},{"location":"server/new_blocks/#full-code-example","title":"Full Code example","text":"<p>Here is the complete implementation of the <code>GetWikipediaSummary</code> blocks:</p> <pre><code>import requests\nfrom autogpt_server.data.block import Block, BlockSchema, BlockOutput\n\nclass GetWikipediaSummary(Block):\n    # Define the input schema with the required field 'topic'\n    class Input(BlockSchema):\n        topic: str  # The topic to get the Wikipedia summary for\n\n    # Define the output schema with the field 'summary'\n    class Output(BlockSchema):\n        summary: str  # The summary of the topic from Wikipedia\n\n    def __init__(self):\n        super().__init__(\n            id=\"h5e7f8g9-1b2c-3d4e-5f6g-7h8i9j0k1l2m\",  # Unique ID for the block\n            input_schema=GetWikipediaSummary.Input,  # Assign input schema\n            output_schema=GetWikipediaSummary.Output,  # Assign output schema\n\n            # Provide sample input and output for testing the block\n\n            test_input={\"topic\": \"Artificial Intelligence\"},\n            test_output={\"summary\": \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"},\n        )\n\n    def run(self, input_data: Input) -&gt; BlockOutput:\n        try:\n            # Make the request to Wikipedia API\n            response = requests.get(f\"https://en.wikipedia.org/api/rest_v1/page/summary/{input_data.topic}\")\n            response.raise_for_status()\n            summary_data = response.json()\n\n            # Output the summary\n            yield \"summary\", summary_data['extract']\n\n        except requests.exceptions.HTTPError as http_err:\n            raise ValueError(f\"HTTP error occurred: {http_err}\")\n        except requests.RequestException as e:\n            raise ValueError(f\"Request to Wikipedia API failed: {e}\")\n        except KeyError as e:\n            raise ValueError(f\"Error processing Wikipedia data: {e}\")\n</code></pre>"},{"location":"server/new_blocks/#key-points-to-remember","title":"Key Points to Remember","text":"<ul> <li>Unique ID: Give your block a unique ID in the <code>__init__</code> method.</li> <li>Input and Output Schemas: Define clear input and output schemas.</li> <li>Error Handling: Implement error handling in the <code>run</code> method.</li> <li>Output Results: Use <code>yield</code> to output results in the <code>run</code> method.</li> <li>Register the Block: Add your new block to the <code>__init__.py</code> file to make it available to the server.</li> <li>Testing: Provide test input and output in the <code>__init__</code> method for automatic testing.</li> </ul> <p>By following these steps, you can create new blocks that extend the functionality of the AutoGPT Agent Server.</p>"}]}